[{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and fundamental services (Cloud Fundamentals, IAM, Budget, Support).\nWeek 2: Learning basic VPC and foundational networking concepts.\nWeek 3: Advanced EC2 inside VPC, NAT Gateway, Security Group, DNS Resolver.\nWeek 4: VPC Peering, Transit Gateway, and complex VPC-to-VPC connectivity.\nWeek 5: Compute Services: EC2, Auto Scaling, Backup, and Storage Gateway.\nWeek 6: Advanced Storage: S3, Glacier, FSx, and Storage Gateway.\nWeek 7: Advanced IAM, AWS Organizations, Identity Center, KMS.\nWeek 8: Database Services, ETL (Kinesis/Glue/Athena), DMS Migration.\nWeek 9: Designing the overall Serverless system architecture: API Gateway, Lambda, DynamoDB, CloudFront, WAF, Cognito, CI/CD pipeline.\nWeek 10: Designing the overall Serverless system architecture: API Gateway, Lambda, DynamoDB, CloudFront, WAF, Cognito, CI/CD pipeline.\nWeek 11: Completing UI, integrating APIs, and preparing for AWS deployment.\nWeek 12: Learning AWS Console, AWS CLI, IAM, EC2, and cloud fundamentals before deploying the system.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Enhance AI-assisted development with Amazon ECS, Amazon EKS and AWS Serverless MCP server by: Elizabeth Fuentes\non 29 May 2025\nin: Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, Amazon Q, Amazon Q Developer, AWS Lambda, Compute, Featured, Launch, News, Serverless\nToday, we’re introducing specialized Model Context Protocol (MCP) servers for Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS) and AWS Serverless, now available in the AWS Labs GitHub. These open source solutions extend AI development assistants capabilities with real-time, contextual responses that go beyond their pre-trained knowledge. While (Large Language Models - LLM) within AI assistants rely on public documentation, MCP servers deliver current context and service-specific guidance to help you prevent common deployment errors and provide more accurate service interactions.\nYou can use these open source solutions to develop applications faster, using up-to-date knowledge of Amazon Web Services (AWS) capabilities and configurations during the build and deployment process. Whether you’re writing code in your integrated development environment - IDE) or debugging production issues, these MCP servers support AI code assistants with deep understanding of Amazon ECS, Amazon EKS, and AWS Serverless capabilities, accelerating the journey from code to production. They work with popular AI-enabled IDEs, including Amazon Q Developer trên command line (CLI),to help you build and deploy applications using natural language commands.\nThe Amazon ECS MCP Server containerizes and deploys applications to Amazon ECS within minutes by configuring all relevant AWS resources, including load balancers, networking, auto-scaling, monitoring, Amazon ECS task definitions, and services. Using natural language instructions, you can manage cluster operations, implement auto-scaling strategies, and use real-time troubleshooting capabilities to identify and resolve deployment issues quickly.\nFor Kubernetes environments, the Amazon EKS MCP Server provides AI assistants with up-to-date, contextual information about your specific EKS environment. It offers access to the latest EKS features, knowledge base, and cluster state information. This gives AI code assistants more accurate, tailored guidance throughout the application lifecycle, from initial setup to production deployment.\nThe AWS Serverless MCP Server enhances the serverless development experience by providing AI coding assistants with comprehensive knowledge of serverless patterns, best practices, and AWS services. Using **AWS Serverless Application Model Command Line Interface (AWS SAM CLI)**integration, you can handle events and deploy infrastructure while implementing proven architectural patterns. This integration streamlines function lifecycles, service integrations, and operational requirements throughout your application development process. The server also provides contextual guidance for infrastructure as code decisions, AWS Lambda specific best practices, and event schemas for AWS Lambda event source mappings.\nLet’s see it in action If this is your first time using AWS MCP servers, visit the Installation and Setup guide trong AWS Labs GitHub repository to installation instructions. Once installed, add the following MCP server configuration to your local setup:\nInstall Amazon Q for command line (CLI) and add the conﬁguration to ~/.aws/amazonq/mcp.json. Nyou’re already an Amazon Q CLI user, add only the configuration.\nJSON { \u0026#34;mcpServers\u0026#34;: { \u0026#34;awslabs.aws-serverless-mcp\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;awslabs.aws-serverless_mcp_server@latest\u0026#34;] }, \u0026#34;awslabs.ecs-mcp-server\u0026#34;: { \u0026#34;disabled\u0026#34;: false, \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;awslabs.ecs-mcp-server@latest\u0026#34;] }, \u0026#34;awslabs.eks-mcp-server\u0026#34;: { \u0026#34;disabled\u0026#34;: false, \u0026#34;command\u0026#34;: \u0026#34;uv\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;awslabs.eks-mcp-server@latest\u0026#34;] } } } For this demo I’m going to use the Amazon Q CLI to create an application that understands video using 02_using_converse_api.ipynb from Amazon Nova model cookbook repository as sample code. To do this, I send the following prompt:.\nI want to create a backend application that automatically extracts metadata and understands the content of images and videos uploaded to an S3 bucket and stores that information in a database. I\u0026#39;d like to use a serverless system for processing. Could you generate everything I need, including the code and commands or steps to set up the necessary infrastructure, for it to work from start to finish? - Use 02_using_converse_api.ipynb as example code for the image and video understanding. Amazon Q CLI identifies the necessary tools, including the MCP server awslabs.aws-serverless-mcp-server. Through a single interaction, the AWS Serverless MCP server determines all requirements and best practices for building a robust architecture.\nI ask to Amazon Q CLI that build and test the application, but encountered an error. Amazon Q CLI quickly resolved the issue using available tools. I verified success by checking the record created in the Amazon DynamoDB table and testing the application with the dog2.jpeg.\nTo enhance video processing capabilities, I decided to migrate my media analysis application to a containerized architecture. I used this prompt:\nI\u0026#39;d like you to create a simple application like the media analysis one, but instead of being serverless, it should be containerized. Please help me build it in a new CDK stack. Amazon Q Developer begins building the application. I took advantage of this time to grab a coffee. When I returned to my desk, coffee in hand, I was pleasantly surprised to find the application ready. To ensure everything was up to current standards, I simply asked:\nplease review the code and all app using the awslabsecs_mcp_server tools Amazon Q Developer CLI gives me a summary with all the improvements and a conclusion.\nI ask it to make all the necessary changes, once ready I ask Amazon Q developer CLI to deploy it in my account, all using natural language.\nAfter a few minutes, I review that I have a complete containerized application from the S3 bucket to all the necessary networking.\nI ask Amazon Q developer CLI to test the app send it the-sea.mp4 and received a timed out error, so Amazon Q CLI decides to use the fetch_task_logs from awslabsecs_mcp_server tool to review the logs, identify the error and then fix it.\nAfter a new deployment, I try it again, and the application successfully processed the video file\nI can see the records in my Amazon DynamoDB table.\nTo test the Amazon EKS MCP server, I have code for a web app in the auction-website-main folder and I want to build a web robust app, for that I asked Amazon Q CLI to help me with this prompt:\nCreate a web application using the existing code in the auction-website-main folder. This application will grow, so I would like to create it in a new EKS cluster Once the Docker file is created, Amazon Q CLI identifies generate_app_manifests from awslabseks_mcp_server as a reliable tool to create a Kubernetes manifests for the application.\nThen create a new EKS cluster using the manage_eks_staks tool.\nOnce the app is ready, the Amazon Q CLI deploys it and gives me a summary of what it created.\nI can see the cluster status in the console.\nAfter a few minutes and resolving a couple of issues using the search_eks_troubleshoot_guide tool the application is ready to use.\nNow I have a Kitties marketplace web app, deployed on Amazon EKS using only natural language commands through Amazon Q CLI.\nGet started today Visit AWS Labs GitHub repository to start using these AWS MCP servers and enhance your AI-powered developmen there. The repository includes implementation guides, example configurations, and additional specialized servers to run AWS Lambda function, which transforms your existing AWS Lambda functions into AI-accessible tools without code modifications, and Amazon Bedrock Knowledge Bases Retrieval MCP server,which provides seamless access to your Amazon Bedrock knowledge bases. Other AWS specialized servers in the repository include documentation, example configurations, and implementation guides to begin building applications with greater speed and reliability.\nTo learn more about MCP Servers for AWS Serverless and Containers and how they can transform your AI-assisted application development, visit the Introducing AWS Serverless MCP Server: AI-powered development for modern applications, Automating AI-assisted container deployments with the Amazon ECS MCP Server, Accelerating application development with the Amazon EKS MCP server deep-dive— Eli\nAbout author Elizabeth Fuentes\nMy mission is to break down complex concepts into easily digestible explanations, inspiring developers to continually expand their skills and knowledge. Through conferences, tutorials, and online resources, I share my expertise with the global developer community, providing them with the tools and confidence to reach their full potential. With a hands-on approach and a commitment to simplifying the complex, I strive to be a catalyst for growth and learning in the world of AWS technology. "},{"uri":"https://tuananh3232.github.io/aws_intership/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Scaling Data Analytics Containers with Event-based Lambda Functions by: Brian Maguire, Guy Bachar, and Mike Boyar\non 30 Aug 2021\nDanh mục: Amazon API Gateway, Amazon EC2, Amazon Elastic Container Service, Analytics, Architecture, AWS Elastic Beanstalk, Intermediate (200), Manufacturing\nThe marketing industry collects and uses data from various stages of the customer journey. When they analyze this data, they establish metrics and develop actionable insights that are then used to invest in customers and generate revenue.\nIf you’re a data scientist or developer in the marketing industry, you likely often use containers for services like collecting and preparing data, developing machine learning models, and performing statistical analysis. Because the types and amount of marketing data collected are quickly increasing, you’ll need a solution to manage the scale, costs, and number of required data analytics integrations.\nIn this post, we provide a solution that can perform and scale with dynamic traffic and is cost optimized for on-demand consumption. It uses synchronous container-based data science applications that are deployed with asynchronous container-based architectures on AWS Lambda. This serverless architecture automates data analytics workflows utilizing event-based prompts.\nSynchronous container applications Data science applications are often deployed to dedicated container instances, and their requests are routed by an Amazon API Gateway or load balancer. Typically, an Amazon API Gateway routes HTTP requests as synchronous invocations to instance-based container hosts.\nThe target of the requests is a container-based application running a machine learning service (SciLearn). The service container is configured with the required dependency packages such as scikit-learn, pandas, NumPy and SciPy.\nContainers are commonly deployed on different targets such as on-premises, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Compute Cloud (Amazon EC2) and AWS Elastic Beanstalk These services run synchronously and scale through Amazon Auto Scaling groups and a time-based consumption pricing model.\nFigure 1. Synchronous container applications diagram\nChallenges with synchronous architectures When using a synchronous architecture, you will likely encounter some challenges related to scale, performance, and cost:\nOperation blocking: The sender does not proceed until Lambda returns, and failure processing, such as retries, must be handled by the sender.\nNo native AWS service integrations: You cannot use several native integrations with other AWS services such as Amazon Simple Storage Service (Amazon S3), Amazon EventBridge and Amazon Simple Queue Service (Amazon SQS).\nIncreased expense: A 24/7 environment can result in increased expenses if resources are idle, not sized appropriately, and cannot automatically scale.\nThe “New for AWS Lambda – Container Image Support” blog post offers a serverless, event-based architecture to address these challenges. This approach is explained in detail in the following section.\nBenefits of using Lambda Container Image Support In our solution, we refactored the synchronous SciLearn application that was deployed on instance-based hosts as an asynchronous event-based application running on Lambda. This solution includes the following benefits:\nEasier dependency management with Dockerfile: Dockerfile allows you to install native operating system packages and language-compatible dependencies or use enterprise-ready container images.\nSimilar tooling: The asynchronous and synchronous solutions use Amazon Elastic Container Registry (Amazon ECR) to store application artifacts. Therefore, they have the same build and deployment pipeline tools to inspect Dockerfiles. This means your team will likely spend less time and effort learning how to use a new tool.\nPerformance and cost: Lambda provides sub-second autoscaling that’s aligned with demand. This results in higher availability, lower operational overhead, and cost efficiency.\nIntegrations: AWS provides more than 200 service integrations to deploy functions as container images on Lambda, without having to develop it yourself so it can be deployed faster.\nLarger application artifact up to 10 GB: This includes larger application dependency support, giving you more room to host your files and packages in oppose to hard limit of 250 MB of unzipped files for deployment packages.\nScaling with asynchronous events AWS offers two ways to asynchronously scale processing independently and automatically: Elastic Beanstalk worker environments and asynchronous invocation with Lambda. Both options offer the following:\nThey put events in an SQS queue.\nThey can be designed to take items from the queue only when they have the capacity available to process a task. This prevents them from becoming overwhelmed.\nThey offload tasks from one component of your application by sending them to a queue and process them asynchronously.\nThese asynchronous invocations add default, tunable failure processing and retry mechanisms through “on failure” and “on success” event destinations, as described in the following section.\nIntegrations with multiple destinations “On failure” and “on success” events can be logged in an SQS queue, Amazon Simple Notification Service (Amazon SNS) topic, EventBridge event bus, or another Lambda function. All four are integrated with most AWS services.\n“On failure” events are sent to an SQS dead-letter queue because they cannot be delivered to their destination queues. They will be reprocessed them as needed, and any problems with message processing will be isolated.\nFigure 2 shows an asynchronous Amazon API Gateway that has placed an HTTP request as a message in an SQS queue, thus decoupling the components.\nThe messages within the SQS queue then prompt a Lambda function. This runs the machine learning service SciLearn container in Lambda for data analysis workflows, which are integrated with another SQS dead letter queue for failures processing.\nHình 2. Example asynchronous-based container applications diagram\nWhen you deploy Lambda functions as container images, they benefit from the same operational simplicity, automatic scaling, high availability, and native integrations. This makes it an appealing architecture for our data analytics use cases.\nDesign considerations The following can be considered when implementing Docker Container Images with Lambda:\nLambda supports container images that have manifest files that follow these formats:\n+ Docker image manifest V2, schema 2 (Docker version 1.10 and newer)\n+ Open Container Initiative (OCI) specifications (v1.0.0 and up)\nStorage in Lambda:\n+ Memory (128 MB to 10,240 MB)\n+ /tmp space (512 MB)\n+ Container images up to10 GB\n+ Amazon S3 or Amazon Elastic File System (Amazon EFS) làm lưu trữ ngoài. as external storage.\nOn create/update, Lambda will cache the image to speed up the cold start of functions during execution. Cold starts occur on an initial request to Lambda, which can lead to longer startup times. The first request will maintain an instance of the function for only a short time period. If Lambda has not been called during that period, the next invocation will create a new instance.\nFine grain role policies are highly recommended for security purposes.\nContainer images can use the Lambda Extensions to integrate monitoring, security and other tools with the Lambda execution environment.\nConclusion We were able to architect this synchronous service based on a previously deployed on instance-based hosts and design it to become asynchronous on Amazon Lambda.\nBy using the new support for container-based images in Lambda and converting our workload into an asynchronous event-based architecture, we were able to overcome these challenges:\nPerformance and security:With batch requests, you can scale asynchronous workloads and handle failure records using SQS Dead Letter Queues and Lambda destinations. Using Lambda to integrate with other services (such as EventBridge and SQS) and using Lambda roles simplifies maintaining a granular permission structure. When Lambda uses an Amazon SQS queue as an event source, it can scale up to 60 more instances per minute, with a maximum of 1,000 concurrent invocations.\nCost optimization: Compute resources are a critical component of any application architecture. Overprovisioning computing resources and operating idle resources can lead to higher costs. Because Lambda is serverless, it only incurs costs on when you invoke a function and the resources allocated for each request.\nBy author Elizabeth Fuentes\nBrian Maguire là Principal Solutions Architect tại Amazon Web Services, nơi anh tập trung giúp khách hàng xây dựng ý tưởng của họ trên đám mây. Anh là một technologist, writer, teacher và student yêu thích việc học hỏi. Brian là đồng tác giả của cuốn sách Scalable Data Streaming with Amazon Kinesis. Elizabeth Fuentes\nGuy là Senior Solutions Architect tại AWS. Anh chuyên hỗ trợ người dùng trong Capital Markets và FinTech với các hành trình chuyển đổi lên cloud. Chuyên môn của anh bao gồm quản lý Danh tính (Identity), bảo mật (security) và truyền thông hợp nhất (unified communication). Elizabeth Fuentes\nMike Boyar là Solutions Architect tại AWS. Trong thời gian rảnh, anh thích nướng bánh mì kiểu French country. "},{"uri":"https://tuananh3232.github.io/aws_intership/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Amazon OpenSearch Serverless: Cost-Effective Search at Any Scale Authors: Satish Nandi and Jon Handler\nPublished: August 2, 2024\nCategories:\nAmazon OpenSearch Service,\nAnnouncements,\nAWS Big Data,\nFoundational (100),\nPrice Reduction\nWe are excited to announce a lower entry-level cost for Amazon OpenSearch Serverless. With support for half (0.5) OpenSearch Compute Units (OCUs) for indexing and search workloads, the entry cost has been reduced by 50%. OpenSearch Serverless is a serverless deployment option for Amazon OpenSearch Service that allows you to run search and analytics workloads without managing infrastructure, shard tuning, or data lifecycle management. It automatically provisions and scales compute resources to provide consistent ingestion performance and millisecond-level query response times—even as your application usage patterns and requirements change.\nOpenSearch Serverless provides three collection types to meet your needs: time-series, search, and vector. The new lower entry cost benefits all collection types. Vector collections, in particular, have emerged as the dominant workload when using OpenSearch Serverless as a knowledge base for Amazon Bedrock. By introducing the 0.5 OCU baseline, the cost of small vector workloads is reduced by half. Time-series and search collections also benefit, especially for small workloads such as proof-of-concept (PoC) deployments and development/test environments.\nA full OCU consists of 1 vCPU, 6 GB RAM, and 120 GB of storage. A half OCU provides 0.5 vCPU, 3 GB RAM, and 60 GB storage. OpenSearch Serverless scales from the first half OCU to a full OCU, then continues scaling in increments of one full OCU. Each OCU also uses Amazon Simple Storage Service (Amazon S3) as its backing store; you pay for S3 storage regardless of OCU size. The number of OCUs required depends on your collection type and your ingestion and search workloads. We will go deeper into these details and compare how the new 0.5 OCU baseline delivers cost benefits.\nOpenSearch Serverless separates compute resources for indexing and search, deploying distinct sets of OCUs for each compute need. You can deploy OpenSearch Serverless in two ways:\nRedundant deployment for production, and Non-redundant deployment for development or testing. Note: Redundant deployments provision twice the compute resources for both indexing and search operations.\nDeployment Types in OpenSearch Serverless The diagram below illustrates the architecture of OpenSearch Serverless in redundant mode.\nIn redundant mode, OpenSearch Serverless deploys two base OCUs for each compute group (indexing and search) across two Availability Zones. For small workloads under 60 GB, OpenSearch Serverless uses a half OCU as the baseline size. The minimum deployment equals four base units—two for indexing and two for search. This corresponds to approximately $350 per month (four half OCUs). All pricing is based on the US-East (N. Virginia) Region and assumes a 30-day month. Under normal operation, all OCUs actively serve traffic, and OpenSearch Serverless scales from this baseline as needed.\nFor non-redundant deployments, OpenSearch Serverless deploys a single base OCU for each compute group, costing approximately $174 per month (two half OCUs).\nRedundant deployments are recommended for production to ensure high availability. If an Availability Zone fails, the remaining zone can still serve traffic. Non-redundant deployments are suitable for development and testing environments to reduce cost. In both configurations, you can set an upper limit on OCU usage to control costs. The system will scale up to—but never beyond—this limit during peak usage.\nCollections in OpenSearch Serverless and Resource Allocation OpenSearch Serverless uses OCUs differently depending on your collection type and stores your data in Amazon S3. When ingesting data, the service writes to both local OCU disk and Amazon S3 before acknowledging the request, ensuring durability and performance. Depending on the collection type, it also stores data in local OCU storage and scales as needed.\nTime-series collections are designed for cost efficiency by limiting how much data is stored locally while offloading older data to Amazon S3. The required number of OCUs depends on data volume and retention period. The system uses whichever is greater: the default minimum OCUs or the minimum OCUs required to store the most recent data per the OpenSearch Serverless data lifecycle policy.\nExample:\nIf you ingest 1 TiB per day and have a 30-day retention period, the “recent data size” is 1 TiB. You would need:\n20 OCUs for indexing (10 OCUs × 2 for redundancy) 20 OCUs for search (10 OCUs × 2) (Each OCU provides 120 GiB of local disk storage.)\nQuerying older data stored only in S3 will increase query latency. This is the trade-off between cost efficiency and older data retrieval times.\nVector collections use RAM to store vector graphs and disk to store indexes. Vector collections keep index data in local OCU memory. RAM limits are reached faster than disk limits, making vector workloads RAM-bound.\nWhen using a full OCU, resource allocation typically includes:\n2 GB for the operating system 2 GB for Java heap 2 GB for vector graph memory 120 GB of local disk for OpenSearch indexes RAM requirements for a vector graph depend on vector dimensionality, vector count, and the algorithm chosen. Refer to “Choose the k-NN algorithm for your billion-scale use case with OpenSearch” for formulas and guidelines.\nNote: System behavior described here reflects the state as of June 2024. Ongoing improvements may further reduce costs.\nSupported AWS Regions The new minimum OCU size (0.5 OCU) is available in all AWS Regions where OpenSearch Serverless is supported.\nSee the AWS Regional Services List for service availability details and documentation to learn more about OpenSearch Serverless.\nConclusion The introduction of 0.5 OCUs significantly reduces the baseline cost of Amazon OpenSearch Serverless. If you have small datasets or light workloads, you can now operate at a much lower cost. Combined with its simplicity and ability to handle search and analytics workloads without infrastructure management, OpenSearch Serverless provides seamless operations even as your traffic and usage patterns evolve.\nAbout the Authors Satish Nandi\nSatish Nandi is a Senior Product Manager for Amazon OpenSearch Service, focusing on OpenSearch Serverless and Geospatial. He has years of experience in computer networking, security, Machine Learning (ML), and Artificial Intelligence (AI). He holds a BEng in Computer Science and an MBA in Entrepreneurship. In his free time, he enjoys flying airplanes, paragliding, and motorcycling. Jon Handler\nJon Handler is a Senior Solutions Architect at AWS. He supports Capital Markets and FinTech customers in their cloud transformation journeys. His areas of expertise include identity management, security, and unified communication systems. "},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day 2025 1. Event Information Event Name: Vietnam Cloud Day 2025 Time: 7:35 – 16:30 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended the event to update myself on the latest trends in Cloud Computing, Generative AI (GenAI), and how AWS is building the technical, data, and security foundations that enable organizations to adopt AI effectively.\nAdditionally, I wanted to understand how business leaders shape AI strategy, contributing to my own career direction.\n3. Agenda Summary 7:35 – 9:00 — Check-in Participants completed check-in and interacted before the official program began.\n9:00 – 9:20 — Opening Opening remarks by a Government representative, emphasizing the role of digital technology and AI in national digital transformation strategies.\n9:20 – 9:40 — Keynote Address Eric Yeo – Country General Manager, AWS Vietnam, Cambodia, Laos \u0026amp; Myanmar\nKey points:\nAWS’s long-term investment commitment in Vietnam Accelerating business growth through Cloud \u0026amp; GenAI Strong regional technology talent potential 9:40 – 10:00 — Customer Keynote 1 Dr. Jens Lottner – CEO, Techcombank\nHighlights:\n“AI-first” strategy in the banking sector Building internal data and AI capabilities Vision to become a technology-led bank 10:00 – 10:20 — Customer Keynote 2 Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network\nContent:\nCombining Blockchain \u0026amp; AI to create innovative products Building an emerging Web3 ecosystem Developing a startup community 10:20 – 10:50 — AWS Keynote Jaime Valles – VP \u0026amp; GM, Asia Pacific \u0026amp; Japan, AWS\nFocused on:\nGenAI as the core of next-generation applications AWS’s comprehensive AI service ecosystem Sustainable and secure technology development models 11:00 – 11:40 — Panel Discussion Navigating the GenAI Revolution: Strategies for Executive Leadership\nModerator: Jeff Johnson (AWS)\nPanelists:\nVu Van – CEO, ELSA Corp Nguyen Hoa Binh – Chairman, NextTech Dieter Botha – CEO, TymeX Key insights:\nExecutive leadership thinking in the GenAI era Building a culture of innovation and embracing change Aligning AI initiatives with business objectives instead of chasing trends 13:30 – 14:00 — Building a Unified Data Foundation on AWS Kien Nguyen – Solutions Architect, AWS\nStrategies for building a unified data foundation Ingestion → Storage → Processing → Governance Preparing data for large-scale Analytics \u0026amp; AI 14:00 – 14:30 — GenAI Adoption \u0026amp; Roadmap on AWS Jun Kai Loke \u0026amp; Tamelly Lim – AWS\nContent:\nGlobal GenAI trends AWS roadmap: from infrastructure to model layers Key AI services: Bedrock, SageMaker, Agents… 14:30 – 15:00 — AI-Driven Development Lifecycle (AI-DLC) Binh Tran – Senior Solutions Architect, AWS\nNew SDLC model with AI as a central collaborator AI-driven execution, human supervision Accelerated development and improved product quality 15:30 – 16:00 — Securing Generative AI Applications on AWS Taiki Dang – Solutions Architect, AWS\nSecurity challenges across three layers: Infrastructure – Model – Application Zero-trust architecture, IAM, encryption by default Protecting data throughout the AI lifecycle 16:00 – 16:30 — Beyond Automation: AI Agents as Productivity Multipliers Michael Armentano – Principal WW GTM Specialist, AWS\nAI Agents becoming “digital employees” Self-learning, adaptive, and autonomous execution Future productivity multiplied by AI Key Takeaways AWS GenAI strategy from leadership to technical perspectives AI-DLC as the future of software development Data architecture required for AI/ML Multi-layer security for Generative AI Potential of AI Agents in enterprises Strategic thinking in AI and the ability to connect Cloud – Data – AI Practical Application Applying AI-DLC concepts into software development and DevOps Connecting lessons in data \u0026amp; infrastructure to AI/ML projects Career direction aligned with Cloud \u0026amp; AI development Preparing necessary skills for real-world AI/ML projects Applying multi-layer security knowledge in system design Event Experience The event provided a comprehensive perspective on GenAI, Cloud, and enterprise AI deployment strategies.\nKeynotes from AWS executives and business leaders helped me better understand how AI is operated in real environments and the infrastructure, data, and security requirements involved.\nHighlights Content: Comprehensive, high quality, and fully updated by AWS Speakers: Senior AWS experts and leaders from top enterprises Organization: Professional and seamless Value: Deep insights and strong strategic direction for Cloud \u0026amp; AI Important Lessons GenAI requires a strong data and security foundation to be successful The AI-DLC model transforms traditional software development AI Agents will become essential for enterprise productivity AI implementation must align with business objectives and system architecture Event Images "},{"uri":"https://tuananh3232.github.io/aws_intership/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nghiêm Tuấn Anh\nPhone Number: 0906872175\nEmail: anhntse183220@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS092025\nInternship Company: AMAZON WEB SERVICES VIETNAM COMPANY LIMITED\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 8/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Learn foundational concepts about cloud computing and AWS. Get familiar with IAM (user, group, MFA) and cost management. Practice creating an AWS account, enabling MFA, creating users, setting budgets, and working with AWS Support. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Review Module 01 theory to understand the overall concepts of cloud and AWS - Learn about AWS Global Infrastructure, Regions, AZs - Learn basic IAM and how AWS manages users/groups 08/09/2025 08/09/2025 https://youtu.be/HxYZAK1coOI?si=vi7dp01LjqDvyIF4 https://youtu.be/IK59Zdd1poE?si=n9pSqe-XCENDsR9B https://youtu.be/HSzrWGqo3ME?si=enh1H3F44IYXuziO https://youtu.be/pjr5a-HYAjI?si=NvVriAEQGRto2QPs https://youtu.be/2PQYqH_HkXw?si=eDL8iYkYSYb7Kcvk 3 - Hands-on: + Create AWS Account + Enable MFA + Create admin group \u0026amp; admin user + Validate account authentication 09/09/2025 09/09/2025 https://youtu.be/IY61YlmXQe8?si=lle8PWNXwvrGvQlN https://youtu.be/Hku7exDBURo?si=IkRmV8fhdqIEac5Z https://000001.awsstudygroup.com 4 - Learn AWS Budget \u0026amp; Cost Management - Hands-on: + Create Budget types (Cost / Usage / RI / Savings Plan) + Clean up after creating Budgets 10/09/2025 10/09/2025 https://000007.awsstudygroup.com 5 - Learn AWS Support - Review support plans - Track and manage support cases 11/09/2025 11/09/2025 https://000009.awsstudygroup.com 6 - Review the entire Module 01 - Double-check IAM users/groups - Validate MFA, budgets, and support functionality - Take notes on key points to prepare for Module 02 12/09/2025 12/09/2025 Week 1 Achievements: Overview:\nThis week I became familiar with the fundamental concepts of cloud computing and AWS, understood how AWS organizes infrastructure, manages services, and optimizes cost. I also practiced account creation, IAM setup, and Budget configuration. Theory learned:\nCloud computing concepts and AWS operating models Global infrastructure: Region, AZ, Edge Location AWS management tools (Console, CLI, SDK) Basic IAM: User, Group, Policy Cost control: Budget, Cost Explorer, Support Plans Hands-on labs:\nCreated AWS account + enabled MFA Created admin group and admin user Created and configured different Budget types Learned AWS Support plans and practiced creating support cases "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.8-cognito/5.8.1-create-user-pool/","title":"Create Cognito User Pool","tags":[],"description":"","content":"Create User Pool In this step, you will create a Cognito User Pool to manage user authentication for your application.\nNavigate to AWS Cognito service in the AWS Console Click Create user pool Configure options\nOptions for sign-in identifiers:\nSelect Email (allows users to sign in with email) Uncheck Phone number and Username Self-registration:\n✅ Enable self-registration (allows users to sign up themselves) Required attributes for sign-up:\nSelect email and name as required attributes Click Create user pool Enter User Pool Name\nUser pool name: TaskManagementUserPool Click Create user pool Verify User Pool Creation After successful creation, you should see your User Pool in the Cognito console with default configurations:\nReview Default Configuration Cognito automatically creates the User Pool with default settings:\nSign-in experience:\nSign-in options: Username (can be changed to Email) Password policy: Cognito defaults Multi-factor authentication: Optional Sign-up experience:\nSelf-service sign-up: Enabled Email verification: Enabled Required attributes: email Message delivery:\nEmail provider: Send email with Cognito App integration:\nHosted UI: Not configured (will be configured in later steps) Important Note ⚠️ Warning: Options for sign-in identifiers and required attributes cannot be changed after the User Pool is created. If you need to change these, you must create a new User Pool.\nNote down the User Pool ID from the overview page as you\u0026rsquo;ll need it for the next steps:\nIn the following steps, we will configure detailed features like password policies, email verification, and create App Clients.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Building the TaskHub Platform with the DevSecOps Model on AWS Workshop Introduction In this workshop, you will build the entire TaskHub platform following the AWS Serverless + DevSecOps model, based on real-world architectures widely adopted by modern enterprises to achieve scalability, security, and cost efficiency.\nThe workshop is divided into groups of AWS services, helping you:\nUnderstand the role of each service in a modern serverless architecture. Deploy API Gateway, Lambda, DynamoDB, Cognito, and S3/CloudFront hands-on. Apply DevSecOps practices using CodePipeline, CodeBuild, and CodeGuru. Implement security at both the Backend (KMS, Secrets Manager) and Edge (WAF, Shield). Fully integrate a Next.js frontend with an AWS serverless backend. Architectural Overview In this workshop, you will build all the key components of the TaskHub platform:\nAmazon S3 – Stores the static build of the Next.js application. Amazon CloudFront – Delivers the UI globally with low latency. AWS WAF \u0026amp; AWS Shield – Protect the application from DDoS attacks and OWASP Top 10 threats. Amazon Cognito – Handles user authentication, identity management, and Admin/Member role authorization. Amazon API Gateway – Serves as the entry point for frontend requests. AWS Lambda (Node.js/TypeScript) – Processes all business logic. Amazon DynamoDB – Stores tasks, users, and progress data. AWS KMS – Encrypts data stored in DynamoDB. AWS Secrets Manager – Stores sensitive information and API keys securely. AWS CodePipeline – Automates the entire CI/CD process. AWS CodeBuild – Builds frontend/backend and performs security scans. AWS CodeGuru Reviewer – Analyzes code quality and provides optimization recommendations. AWS CloudFormation – Deploys infrastructure via IaC. Amazon CloudWatch Logs – Captures logs from Lambda and API Gateway. AWS X-Ray – Provides system-wide tracing and latency analysis. Amazon SNS – Sends system events and alert notifications. "},{"uri":"https://tuananh3232.github.io/aws_intership/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Proposal Template Taskhub Proposal Template Taskhub\nTaskHub - Task and Progress Management Platform following the DevSecOps Model on AWS 1. Executive Summary TaskHub is a task and progress management platform designed to help working groups or small to medium-sized businesses manage work, deadlines, and progress in a visual and secure manner.\nThe system is developed following the DevSecOps model, built entirely on AWS Serverless, ensuring scalability, security, and cost optimization.\nThe development and deployment process uses AWS CodePipeline and CodeBuild to automate CI/CD and security testing.\n2. Problem Statement Problem:\nSmall businesses and project teams often struggle with managing workload, tracking progress, and distributing tasks among members. Popular management tools like Jira or Asana often have high costs and lack seamless support for DevSecOps processes or the AWS environment. Solution:\nTaskHub uses a Serverless architecture on AWS to build a lightweight, secure, and cost-effective platform. The platform is developed using AWS Lambda, API Gateway, DynamoDB, Cognito, and S3/CloudFront, while integrating AWS CodePipeline for CI/CD and dynamic security testing. Benefits and Return on Investment (ROI)\nThe TaskHub solution brings many practical benefits for development teams and small to medium-sized businesses. The system serves as a central platform for managing tasks, tracking progress, and delegating authority to members effectively. The application of the serverless model on AWS helps minimize operating costs, optimize resources, and increase scalability as usage demand grows. Furthermore, this platform supports building a practical DevSecOps environment, paving the way for research and development teams to expand further projects. According to estimates from the AWS Pricing Calculator, the system\u0026rsquo;s operating cost is only about $0.66 per month, equivalent to $7.92 per year, while the entire initial infrastructure leverages shared services from AWS, with no additional hardware costs. The expected payback period is achieved within 6-12 months due to significantly reduced manual management work and optimized internal workflow.\n3. Solution Architecture The TaskHub platform is built based on AWS Serverless architecture, ensuring operational scalability, high performance, and cost-effective operation. The system focuses on task management, teamwork, and real-time project progress, while maintaining an automated development and deployment process through the DevSecOps model.\nThe overall architecture includes key components such as Amazon API Gateway taking responsibility for receiving and distributing user requests, AWS Lambda handling business logic backend and interacting with the Amazon DynamoDB database to store task information, users, and access permissions.\nThe web interface is hosted via Amazon S3 and distributed globally by Amazon CloudFront, while AWS Cognito ensures authentication and user authorization.\nThe CI/CD process is automated using AWS CodePipeline combined with AWS CodeBuild, enabling continuous development deployment and security testing without server management.\nThe entire architecture is protected by AWS WAF and AWS KMS to enhance data security and ensure additional DevSecOps compliance. AWS X-Ray is used to monitor performance and analyze latency. The overall architecture is described in detail in the diagram below:\nAWS Services Used Amazon Route 53: Highly reliable DNS service, routing traffic. AWS WAF (Web Application Firewall): Advanced protection layer, blocking common attacks. Amazon CloudFront: Global distribution of user interface and static content. Amazon S3 (Simple Storage Service): Static hosting of the entire web interface source code (Next.js build files). Amazon Cognito: User authentication and authorization management. Amazon API Gateway: Middleware communication layer, performing authentication and routing API requests to Lambda. AWS Lambda: Core business logic processing. Integrated to log activities into CloudWatch Logs. Amazon DynamoDB: High-performance NoSQL database. Data is encrypted using AWS KMS. AWS SNS (Simple Notification Service): Handles asynchronous notifications. AWS Secrets Manager: Secure storage, management, and rotation of secrets. AWS CodePipeline, CodeBuild, \u0026amp; CodeGuru: CodePipeline/CodeBuild: Building and automating the CI/CD process. CodeBuild runs automated tests and Static Application Security Testing (SAST). AWS CodeGuru: Automated source code analysis tool, integrated into the CI/CD process to provide intelligent recommendations for performance optimization and code quality improvement, especially important in the Lambda environment. AWS CloudFormation: Infrastructure as Code (IaC) service for deploying all resources. AWS CloudWatch Logs \u0026amp; AWS X-Ray: CloudWatch Logs collects logs. CloudWatch uses this data to set up alerts. AWS X-Ray provides in-depth request tracing capabilities. Component Design User Interface Layer (Frontend):\nInterface: Next.js application built as static files. Hosting \u0026amp; Distribution: Static files are securely stored in Amazon S3 (configured as Origin for CloudFront). This interface is distributed globally by Amazon CloudFront with low latency, while being protected by AWS WAF (Web Application Firewall) at the Edge layer. Business Logic Layer (Backend):\nAPI Gateway: Amazon API Gateway receives all requests. It is configured with Cognito Authorizer to validate user tokens before forwarding requests. Processing: Lambda Functions are responsible for handling business logic (CRUD tasks, team management, permissions). Secret Management: Each Lambda function accesses sensitive information (such as external API keys) through AWS Secrets Manager, ensuring secrets are never hard-coded. Data Layer (Database):\nDatabase: Amazon DynamoDB is used to store task data, progress, and user configuration. DynamoDB operates in On-Demand mode for automatic scaling and cost optimization. Data Security: All data at rest in DynamoDB is encrypted using keys managed by AWS KMS (Key Management Service), meeting the highest security standards. Security and Authentication:\nAuthentication: Amazon Cognito provides login mechanisms, session management, and Role-Based Access Control (RBAC) for users. Cognito also supports Multi-Factor Authentication (MFA) and SSO (Single Sign-On) integration. Edge Protection: AWS WAF is placed in front of CloudFront to prevent Layer 7 DDoS attacks and other common security vulnerabilities (OWASP Top 10). Deployment and Monitoring:\nCI/CD DevSecOps: Source code is stored on GitLab (as per the diagram) and automated via the AWS CodePipeline/CodeBuild chain. This process includes running CodeGuru to optimize code before deploying infrastructure via CloudFormation. Monitoring \u0026amp; Debugging: AWS CloudWatch Logs collects detailed logs from all services. AWS CloudWatch uses these logs to set up automatic alerts for errors or incidents. AWS X-Ray provides an overview of transaction flow performance, aiding in debugging and latency optimization. 4. Technical Implementation The development of the TaskHub project is divided into two main parts—building the AWS serverless infrastructure and developing the task management platform—each including the following key implementation phases:\nDeclared Development Phases\nPhase 1: Design and Modeling (Month 1)\nMain Action: Research Serverless/DevSecOps, select core services (Lambda, DynamoDB, API Gateway). Design detailed architecture diagrams and NoSQL data models. Deliverables: Architecture Diagram and Data Model Documentation. Phase 2: Infrastructure as Code Initialization (Month 2)\nMain Action: Calculate detailed operating costs. Use AWS CDK to build IaC source code for platform services (S3, CloudFront, Cognito), ensuring environment reproducibility. Deliverables: Base AWS CDK source code and Operating Cost Report. Phase 3: DevSecOps Automation Setup (Month 2-3)\nMain Action: Set up a complete CI/CD Pipeline (CodePipeline/CodeBuild). Integrate AWS CodeGuru and SAST tools to automate source code quality and security checks before deployment. Deliverables: Operational CodePipeline and automated security scanning process. Phase 4: Development and Deployment (Month 3-4)\nMain Action: Develop functionality (Lambda Functions with TypeScript) and interface (Next.js). Perform Integration Testing between services. Deploy the official production release via Pipeline. Deliverables: TaskHub Beta Version (complete CRUD) and Testing Report. Technical Requirements\nArchitecture and Tools: The entire system is declared and managed using AWS CDK to ensure infrastructure consistency. Technology: Backend uses TypeScript/Node.js. Frontend uses Next.js (React). Source Code Management: Source code on GitLab, automated deployment via AWS CodePipeline. Monitoring: Configure CloudWatch, X-Ray, and CloudWatch Logs for performance monitoring and in-depth debugging. Non-functional Requirements: The system is located in Singapore (ap-southeast-1) to optimize speed in Vietnam, capable of scaling up to 50 users, and uses AWS KMS for data encryption. 5. Timeline \u0026amp; Key Milestones Project Timeline Pre-internship (Month 0): Prepare plans, research DevSecOps and AWS Serverless services. Month 1: Set up development environment, initiate AWS infrastructure and CI/CD pipeline. Month 2: Design architecture, develop core functionality, and automate security testing. Month 3: Integrate frontend-backend, develop testing, and launch the platform. Post-launch: Maintenance, performance evaluation, and expansion of advanced features. 6. Budget Estimate Resource Responsibility Rate (USD) / Hour Solution Architects [1 person] System Architecture Design, API design, Database Schema, Technical Leadership 6 Engineers [3 person] Backend Development, Frontend Development, Security Implement 4 Other (DevOps) [1 person] CI/CD, Cloud Deployment, Monitoring, Security, Security Configuration 4 Project Phase Solution Architects Engineers Other (Please specify) Total Hours System Design \u0026amp; Architecture 20 10 0 30 Backend Development 10 80 0 90 Frontend Development 5 60 0 65 Security \u0026amp; CI/CD Setup 5 30 10 45 Testing \u0026amp; Deployment 5 30 0 35 Total Hours 45 210 10 265 Total Cost (USD) 270 840 40 800 Cost Contribution distribution between Partner, Customer, AWS:\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 800 100 7. Risk Assessment Risk Matrix\nNetwork or AWS service disruption: Medium impact, medium probability. CI/CD deployment errors: High impact, low probability. Exceeding AWS budget: Medium impact, low probability. Security vulnerabilities: High impact, medium probability. Performance degradation under load: Medium impact, medium probability. Mitigation Strategies\nUse AWS multi-region and monitor with CloudWatch/X-Ray. Test and review source code before deployment via CodePipeline. Set up cost alerts via AWS Budgets. Perform automated security scanning using CodeBuild (replacing GitHub Actions). Contingency Plan\nMaintain a staging environment for quick recovery. Use CloudFormation and AWS Backup for configuration and data backup. 8. Expected Outcomes Technical Improvements\nComprehensive Automation: Complete transition to automated DevSecOps process. New system deployment time takes under 6 minutes. Performance Guarantee: Fast application operation (API response under 150ms) and stability (99.9% Uptime) thanks to Serverless architecture. Integrated Security: Automated scanning and remediation of high-level security vulnerabilities during the Code Build process. Scalability Ready: The platform can scale to serve many users and handle large traffic volumes without structural changes. Long-term Value\nTechnical Asset Creation: Creation of a complete AWS CDK/CloudFormation codebase. This is a cost-optimized Serverless architecture template that can be reused for other projects by the team. Robust Platform Foundation: Establishment of a work management and development environment following industrial standards (DevSecOps), ready for future feature expansion. "},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: Club Session – \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Purpose of the Session Explore how Generative AI is transforming modern software development. Understand how AI can be fully integrated into the Software Development Life Cycle (SDLC) from architecture to maintenance. Experience hands-on examples of AI-powered development tools such as Amazon Q Developer and Kiro. Speakers \u0026amp; Organizers Main Facilitators: Toan Huynh – Presented AI-Driven SDLC and demoed Amazon Q Developer My Nguyen – Demonstrated Kiro Coordinators: Diem My, Dai Truong, Dinh Nguyen (AWS GenAI Builder Club Vietnam) Key Content \u0026amp; Learnings 1. Overview of the AI-Driven Development Life Cycle Generative AI is reshaping the entire SDLC, not just assisting with small tasks. AI automates repetitive work such as boilerplate coding, generating test cases, and basic debugging. Developers are shifting from writing every line of code to designing, supervising, and collaborating with AI. 2. Amazon Q Developer – An End-to-End AI Developer Assistant Supports all stages of SDLC: planning, requirement analysis, architecture design coding, code explanation, refactoring, language migration debugging and testing deployment guidance and troubleshooting Deep AWS integration enhances productivity for cloud-native workloads. Built with strong privacy principles—enterprise data is not used to train global models. 3. Kiro – Practical AI Pair Programming Understands complex codebases and provides intelligent modification suggestions. Accelerates development by generating accurate, production-ready code. Acts as a learning companion to quickly explore new languages or frameworks. Key Takeaways Trends \u0026amp; Future Direction AI will not replace developers; instead, it elevates their role toward problem-solving, system design, and quality assurance. Speed becomes a competitive advantage for organizations that adopt AI across the SDLC. Skills to Develop Prompt engineering becomes essential: specifying requirements clearly and validating AI-generated output. Architectural thinking and oversight skills are needed to evaluate AI-generated code safely. Strong fundamentals remain crucial: algorithms, data structures, and system design enable developers to guide AI effectively. Personal Action Plan Register and use Amazon Q Developer in VSCode/JetBrains for daily tasks. Practice prompt engineering for coding scenarios—code generation, testing, debugging. Engage actively with the AWS GenAI Builder Club community. Apply an AI-driven development workflow to a real module in a personal or team project. Personal Reflection This club session felt like witnessing the next evolution of software engineering.\nSeeing Amazon Q Developer and Kiro operate in real time—proposing improvements, detecting potential vulnerabilities, and converting natural language prompts into working code—was truly eye-opening.\nThe most impactful message was: “AI empowers, not replaces.”\nDevelopers who learn to collaborate with AI will have a tremendous advantage in the future.\nThis session encouraged me to be proactive in mastering AI tools and to integrate them into my workflow as an essential part of becoming a next-generation software engineer.\nPhotos "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about Amazon VPC and its core components such as Subnet, Internet Gateway, and NAT Gateway. Learn about key security concepts such as Security Group and NACL. Get familiar with launching EC2 instances and basic networking configurations. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Watch the theoretical videos of Module 2 to get an overall understanding of VPC - Define concepts such as Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, and NACL - Review the VPC architecture model 15/09/2025 15/09/2025 https://youtu.be/O9Ac_vGHquM?si=wDWqr_lJUjK2csDt https://youtu.be/BPuD1l2hEQ4?si=Qaig6saCCVKiqe0H https://youtu.be/CXU8D3kyxIc?si=TiZqHi0uB5mCif3L 3 - Hands-on: + Create VPC + Create Subnet + Configure Route Table for the public subnet + Attach Internet Gateway to the VPC + Verify the route table to confirm IGW functionality 16/09/2025 16/09/2025 https://000003.awsstudygroup.com 4 - Learn about Security Groups in VPC - Learn about Network ACLs - Review inbound/outbound rules of Security Groups - Review NACL rules in the subnet 17/09/2025 17/09/2025 https://000003.awsstudygroup.com 5 - Hands-on: + Launch EC2 instance in the public subnet + Create key pair to connect to EC2 + Review EC2 Security Group + Review EC2 Route Table and subnet mapping 18/09/2025 18/09/2025 https://000003.awsstudygroup.com 6 - Hands-on: + Test SSH connection to EC2 + Final verification of Security Group rules - Summarize all Module 2 theoretical content 19/09/2025 19/09/2025 https://000003.awsstudygroup.com Week 2 Achievements: Overview:\nThis week I became familiar with fundamental networking concepts in AWS and learned how to build a simple VPC. The main focus was understanding how VPC creates an isolated environment, how subnets are divided, and how IGW, routing, and security work together. Theory learned:\nOverview of VPC and how it operates within AWS Subnet (Public / Private) and subnet segmentation Internet Gateway \u0026amp; Route Table Security Group \u0026amp; Network ACL VPC Peering, VPN, Direct Connect Hands-on labs:\nPracticed creating a VPC Created subnets according to the lab design (public/private) Created and attached an Internet Gateway to the VPC Created Route Table and configured routes for the subnets Reviewed VPC structure after deployment "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.8-cognito/5.8.2-password-policies/","title":"Configure Password Policies","tags":[],"description":"","content":"Configure Password Policies In this step, you will configure password policies to ensure strong password requirements for your users.\nNavigate to your User Pool in the Cognito console Go to Authentication methods tab Click Edit in the Password policy section Password Policy Configuration Configure the following password requirements:\nPassword length:\nMinimum length: 8 characters Maximum length: 256 characters Password complexity:\n✅ Require numbers ✅ Require special characters ✅ Require uppercase letters ✅ Require lowercase letters Save Configuration Review your password policy settings Click Save changes Verify Password Policy The password policy is now active. Users will need to create passwords that meet these requirements:\nExample of valid passwords:\nMySecurePass123! StrongPassword2024# TaskManager@2025 Example of invalid passwords:\npassword (no uppercase, numbers, special chars) 12345678 (no letters, special chars) Pass1! (too short) "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3FullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:DeleteBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketCORS\u0026#34;, \u0026#34;s3:PutBucketCORS\u0026#34;, \u0026#34;s3:GetBucketWebsite\u0026#34;, \u0026#34;s3:PutBucketWebsite\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:PutBucketVersioning\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateDistribution\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:GetDistributionConfig\u0026#34;, \u0026#34;cloudfront:UpdateDistribution\u0026#34;, \u0026#34;cloudfront:DeleteDistribution\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;cloudfront:CreateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:GetOriginAccessControl\u0026#34;, \u0026#34;cloudfront:UpdateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:DeleteOriginAccessControl\u0026#34;, \u0026#34;cloudfront:ListOriginAccessControls\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFAndShieldAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:DeleteWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34;, \u0026#34;wafv2:AssociateWebACL\u0026#34;, \u0026#34;wafv2:DisassociateWebACL\u0026#34;, \u0026#34;wafv2:CreateIPSet\u0026#34;, \u0026#34;wafv2:GetIPSet\u0026#34;, \u0026#34;wafv2:UpdateIPSet\u0026#34;, \u0026#34;wafv2:DeleteIPSet\u0026#34;, \u0026#34;wafv2:ListIPSets\u0026#34;, \u0026#34;wafv2:CreateRuleGroup\u0026#34;, \u0026#34;wafv2:GetRuleGroup\u0026#34;, \u0026#34;wafv2:UpdateRuleGroup\u0026#34;, \u0026#34;wafv2:DeleteRuleGroup\u0026#34;, \u0026#34;wafv2:ListRuleGroups\u0026#34;, \u0026#34;shield:DescribeSubscription\u0026#34;, \u0026#34;shield:GetSubscriptionState\u0026#34;, \u0026#34;shield:DescribeProtection\u0026#34;, \u0026#34;shield:ListProtections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;cognito-idp:DescribeUserPool\u0026#34;, \u0026#34;cognito-idp:ListUserPools\u0026#34;, \u0026#34;cognito-idp:UpdateUserPool\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolClient\u0026#34;, \u0026#34;cognito-idp:UpdateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:ListUserPoolClients\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-identity:CreateIdentityPool\u0026#34;, \u0026#34;cognito-identity:DeleteIdentityPool\u0026#34;, \u0026#34;cognito-identity:DescribeIdentityPool\u0026#34;, \u0026#34;cognito-identity:UpdateIdentityPool\u0026#34;, \u0026#34;cognito-identity:ListIdentityPools\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;APIGatewayFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34;, \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:PUT\u0026#34;, \u0026#34;apigateway:PATCH\u0026#34;, \u0026#34;apigateway:DELETE\u0026#34;, \u0026#34;apigateway:UpdateRestApiPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:UpdateFunctionConfiguration\u0026#34;, \u0026#34;lambda:PublishVersion\u0026#34;, \u0026#34;lambda:CreateAlias\u0026#34;, \u0026#34;lambda:UpdateAlias\u0026#34;, \u0026#34;lambda:DeleteAlias\u0026#34;, \u0026#34;lambda:GetAlias\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:AddPermission\u0026#34;, \u0026#34;lambda:RemovePermission\u0026#34;, \u0026#34;lambda:GetPolicy\u0026#34;, \u0026#34;lambda:PutFunctionConcurrency\u0026#34;, \u0026#34;lambda:DeleteFunctionConcurrency\u0026#34;, \u0026#34;lambda:TagResource\u0026#34;, \u0026#34;lambda:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:CreateTable\u0026#34;, \u0026#34;dynamodb:DeleteTable\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34;, \u0026#34;dynamodb:DescribeTimeToLive\u0026#34;, \u0026#34;dynamodb:UpdateTimeToLive\u0026#34;, \u0026#34;dynamodb:DescribeContinuousBackups\u0026#34;, \u0026#34;dynamodb:UpdateContinuousBackups\u0026#34;, \u0026#34;dynamodb:TagResource\u0026#34;, \u0026#34;dynamodb:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;KMSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateKey\u0026#34;, \u0026#34;kms:CreateAlias\u0026#34;, \u0026#34;kms:DeleteAlias\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListKeys\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:PutKeyPolicy\u0026#34;, \u0026#34;kms:GetKeyPolicy\u0026#34;, \u0026#34;kms:EnableKey\u0026#34;, \u0026#34;kms:DisableKey\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsManagerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:PutSecretValue\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodePipelineAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codepipeline:CreatePipeline\u0026#34;, \u0026#34;codepipeline:DeletePipeline\u0026#34;, \u0026#34;codepipeline:GetPipeline\u0026#34;, \u0026#34;codepipeline:GetPipelineState\u0026#34;, \u0026#34;codepipeline:UpdatePipeline\u0026#34;, \u0026#34;codepipeline:ListPipelines\u0026#34;, \u0026#34;codepipeline:StartPipelineExecution\u0026#34;, \u0026#34;codepipeline:StopPipelineExecution\u0026#34;, \u0026#34;codepipeline:GetPipelineExecution\u0026#34;, \u0026#34;codepipeline:ListPipelineExecutions\u0026#34;, \u0026#34;codepipeline:TagResource\u0026#34;, \u0026#34;codepipeline:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeBuildAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateProject\u0026#34;, \u0026#34;codebuild:DeleteProject\u0026#34;, \u0026#34;codebuild:UpdateProject\u0026#34;, \u0026#34;codebuild:BatchGetProjects\u0026#34;, \u0026#34;codebuild:ListProjects\u0026#34;, \u0026#34;codebuild:StartBuild\u0026#34;, \u0026#34;codebuild:StopBuild\u0026#34;, \u0026#34;codebuild:BatchGetBuilds\u0026#34;, \u0026#34;codebuild:ListBuildsForProject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeGuruReviewerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codeguru-reviewer:AssociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeRepositoryAssociation\u0026#34;, \u0026#34;codeguru-reviewer:ListRepositoryAssociations\u0026#34;, \u0026#34;codeguru-reviewer:DisassociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeCodeReview\u0026#34;, \u0026#34;codeguru-reviewer:ListCodeReviews\u0026#34;, \u0026#34;codeguru-reviewer:ListRecommendations\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFormationAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:UpdateStack\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;cloudformation:GetTemplate\u0026#34;, \u0026#34;cloudformation:ValidateTemplate\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudformation:ListStackResources\u0026#34;, \u0026#34;cloudformation:CreateChangeSet\u0026#34;, \u0026#34;cloudformation:DeleteChangeSet\u0026#34;, \u0026#34;cloudformation:DescribeChangeSet\u0026#34;, \u0026#34;cloudformation:ExecuteChangeSet\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DeleteLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:GetLogEvents\u0026#34;, \u0026#34;logs:FilterLogEvents\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;logs:DeleteRetentionPolicy\u0026#34;, \u0026#34;logs:TagLogGroup\u0026#34;, \u0026#34;logs:UntagLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;XRayAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;xray:PutTraceSegments\u0026#34;, \u0026#34;xray:PutTelemetryRecords\u0026#34;, \u0026#34;xray:GetSamplingRules\u0026#34;, \u0026#34;xray:GetSamplingTargets\u0026#34;, \u0026#34;xray:GetServiceGraph\u0026#34;, \u0026#34;xray:GetTraceSummaries\u0026#34;, \u0026#34;xray:GetTraceGraph\u0026#34;, \u0026#34;xray:BatchGetTraces\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SNSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sns:CreateTopic\u0026#34;, \u0026#34;sns:DeleteTopic\u0026#34;, \u0026#34;sns:GetTopicAttributes\u0026#34;, \u0026#34;sns:SetTopicAttributes\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;sns:Subscribe\u0026#34;, \u0026#34;sns:Unsubscribe\u0026#34;, \u0026#34;sns:ListSubscriptions\u0026#34;, \u0026#34;sns:ListSubscriptionsByTopic\u0026#34;, \u0026#34;sns:Publish\u0026#34;, \u0026#34;sns:TagResource\u0026#34;, \u0026#34;sns:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPassRoleForServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PassedToService\u0026#34;: [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;apigateway.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;cloudformation.amazonaws.com\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMRoleManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:UpdateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:ListAttachedRolePolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/4.3-event3/_.index/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Purpose of the Workshop Provide an overview of the AI/ML/GenAI ecosystem on AWS, focusing on the most important services. Equip attendees with practical knowledge of the AI/ML workflow—from data preparation to model training and deployment using Amazon SageMaker. Introduce the application of Generative AI with Amazon Bedrock, including prompt engineering, RAG, and building automation agents. Create an environment for students and developers to connect and exchange real-world AI/ML experience on the cloud. Speakers / Instructors (According to the workshop program) Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Trần Thị Minh Anh – Senior AI/ML Specialist Solutions Architect, AWS Lê Quang Huy – Cloud Engineer \u0026amp; AI Enthusiast Key Content \u0026amp; Learnings 1. Overview of AWS AI/ML/GenAI Ecosystem Vietnam landscape: AI adoption is growing rapidly; demand for AI/ML and cloud-skilled engineers continues to rise. Learning roadmap: From foundational courses to advanced certifications, emphasizing hands-on practice. Three service groups: AI Services (ready-to-use), ML Services (customizable), and Generative AI (opening new possibilities). 2. Amazon SageMaker – A Complete ML Platform Supports the entire ML lifecycle in a single platform. SageMaker Studio provides an integrated development environment that accelerates model development. Enables MLOps, training automation, hyperparameter tuning, and model deployment. 3. Generative AI with Amazon Bedrock Offers multiple Foundation Models such as Claude, Llama, and Titan for different tasks. Advanced Prompt Engineering: using Chain-of-Thought, few-shot techniques for improved output quality. RAG (Retrieval-Augmented Generation): combines LLMs with private data to reduce hallucination and expand knowledge scope. Bedrock Agents: enable models to perform multi-step tasks and interact with external systems. Guardrails: ensure safe content and compliance. Key Takeaways Technology Insights SageMaker is not just a training tool but a comprehensive ML lifecycle management platform. Effective Generative AI relies on the combination of models, prompt techniques, and supporting architectures like RAG. Data remains the most critical factor influencing model quality. Mindset \u0026amp; Methodology Start with real-world problems instead of chasing technology trends. Apply an iterative approach: fast prototyping → gather feedback → improve. Always evaluate cost to ensure project feasibility. Practical Skills Prompt engineering requires continuous practice. Proficiency in AWS Console/SDK is essential for building real-world AI products. Personal Action Plan After the Workshop Practice using AWS Free Tier — start with SageMaker Studio Lab to explore ML workflows. Build a first GenAI application — for example, creating a simple chatbot using Amazon Bedrock. Deepen learning through AWS Skill Builder — complete learning paths for ML and Generative AI. Join Vietnam AI/ML communities to stay updated with case studies and hands-on knowledge. Personal Experience at the Event The “AI/ML/GenAI on AWS” workshop provided a clearer and more practical perspective on how to approach AI in a professional direction. The modern AWS office environment, the enthusiasm of the speakers, and their easy-to-understand explanations made the content very engaging.\nThe live demo was the most impressive part. Seeing Ms. Minh Anh build a working RAG system and Bedrock Agent in a short time made me realize how accessible AI prototyping has become when using the right tools.\nInsights about cost management and MLOps were especially valuable—they helped me understand that AI is not just about building a model, but also about managing, monitoring, and optimizing it continuously.\nAfter the workshop, I feel more confident in pursuing AI/ML on AWS, as I now have a clearer and more practical roadmap to follow.\nEvent Photos "},{"uri":"https://tuananh3232.github.io/aws_intership/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Enhance AI-assisted development with Amazon ECS, Amazon EKS and AWS Serverless MCP server This blog introduces how to enhance the AI-assisted development experience by using Model Context Protocol (MCP) servers for Amazon ECS, Amazon EKS, and AWS Serverless. You will learn why MCP servers are essential in enabling AI assistants to deeply understand the AWS environment context, thereby providing more accurate coding, deployment, and debugging assistance. The article also explains how to set up and use MCP servers with Amazon Q CLI, illustrated through real-world examples such as building a serverless media analysis app, deploying a containerized application on ECS, and developing a scalable web app on EKS. sThrough this, you will see how MCP-powered AI code assistants can reduce development time, minimize deployment errors, and automate DevOps workflows throughout the entire journey from source code to production\nBlog 2 - Scaling Data Analytics Containers with Event-based Lambda Functions This blog introduces how to scale data analytics containers using AWS Lambda through an event-driven serverless architecture. It explains how to refactor synchronous data science applications into asynchronous ones, enabling automated analytics workflows, lower operational costs, and Lambda’s autoscaling capabilities. The article also highlights the benefits of Lambda container image support, its integration with SQS, EventBridge, and API Gateway, and key design considerations to optimize performance, security, and cost efficiency.\nBlog 3 - Amazon OpenSearch Serverless cost-effective search capabilities, at any scale Amazon OpenSearch Serverless has introduced a lower entry cost with half OpenSearch Compute Units (OCUs) for indexing and search workloads, optimizing costs for small workloads and development environments. OpenSearch Serverless automatically scales without infrastructure management. With three types of collections: Time-series, search, and vector, it reduces costs, especially for small vector workloads. The post also explains deployment options with redundant and non-redundant configurations for different environments.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about EC2 within a VPC. Understand NAT Gateway \u0026amp; network security. Learn how to set up DNS and Route 53. Tasks to be carried out this week: Day Task Start Date End Date References 2 - Review theory about EC2 in a VPC and NAT Gateway - Summarize the flow Public Subnet -\u0026gt; Private Subnet -\u0026gt; NAT -\u0026gt; IGW - Consolidate understanding of how NAT works 22/09/2025 22/09/2025 https://youtu.be/N58agSU4O8o?si=QVRiT5MDt9VN5el4 https://youtu.be/B1qxOQLmavQ?si=3ALVG-HGI0sbL8bM https://youtu.be/GVDsDu9dOFY?si=ki8qJYmPxwxLgYxd 3 - Hands-on: + Launch EC2 instance in the public subnet + Verify Security Group settings + Review subnet mapping and Route Table + Test EC2 Instance Connect 23/09/2025 23/09/2025 https://000003.awsstudygroup.com 4 - Hands-on: + Create NAT Gateway + Check private subnet routes + Test outbound traffic from private subnet to confirm NAT functionality 24/09/2025 24/09/2025 https://000003.awsstudygroup.com 5 - Learn about Hybrid DNS with Route 53 - Hands-on: + Create key pair + Create CloudFormation template 25/09/2025 25/09/2025 https://000010.awsstudygroup.com 6 - Hands-on: + Configure Security Groups + Connect to RDGW + Create Outbound and Inbound Endpoints + Create Resolver Rules and test DNS - Clean up all lab resources - Summarize EC2 – NAT – DNS concepts 26/09/2025 26/09/2025 https://000010.awsstudygroup.com Week 3 Achievements: Overview:\nThis week I learned how EC2 operates inside a VPC, understood the architecture of public/private subnets, NAT Gateway, and the security model. I also practiced extensively with EC2 and networking configurations. Theory learned:\nHow EC2 connects with subnets, route tables, and security groups How NAT Gateway enables Internet connectivity for private subnets How Security Groups control inbound/outbound traffic How NACL filters traffic at the subnet level DNS Resolver and how Route 53 handles hybrid DNS Hands-on labs:\nLaunched EC2 instances in different types of subnets Tested connectivity using EC2 Instance Connect Created NAT Gateway and verified outbound connections Continued practicing through DNS lab exercises to reinforce concepts "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.3-lambda/","title":"Lambda","tags":[],"description":"","content":"Initializing Lambda Create Lambda Function Select Create function → Author from scratch.\nIn the Basic information section, configure:\nFunction name: taskhub-backend-1 Runtime: .NET 8 (C#/F#/Powershell) Architecture: arm64 (default) In Permissions → Change default execution role:\nSelect Create a new role with basic Lambda permissions\n(AWS will automatically create a role with CloudWatch Logs write access) Keep the remaining settings as default and click Create function to finish.\nBuild Backend \u0026amp; Package into ZIP Because Lambda does not allow direct code editing for the .NET runtime, you must upload a ZIP file.\nOn your local machine, build the backend using: dotnet publish -c Release -o publish Open the publish folder → Select all files inside it, do not select the folder itself.\nCompress them into backend.zip Upload Source Code to Lambda In the Code tab → select Upload from → .zip file Select the file backend.zip\nClick Upload and wait for Lambda to deploy\nAfter the upload completes, the Code properties section will display the package size, SHA256 hash, and the last updated timestamp.\nVerify Runtime Settings In the Runtime settings section:\nRuntime: .NET 8 Handler: YourProject::YourNamespace.YourHandler::FunctionHandler\n(depends on your project structure) Architecture: arm64 Ensure that Lambda is running the correct entry point.\nConfigure Environment Variables Open the Configuration tab → Environment variables Click Edit → Add environment variable Add example variables:\nASPNETCORE_ENVIRONMENT = Production DynamoDB_TableName = Your_dynamoDB Jwt_Secret = Your_JWT_Secret Click Save.\nConfigure Timeout \u0026amp; Memory Open Configuration → General configuration\nClick Edit\nSet:\nMemory: 512MB – 1024MB Timeout: 30s (suitable for APIs) or higher if needed Click Save.\nAdd Trigger (API Gateway) To expose Lambda as an API with a public URL:\nGo to Configuration → Triggers Click Add trigger Choose:\nAPI Gateway Create an API REST API Security: Open (or IAM/Authorizer depending on your system)\nClick Add\n"},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.8-cognito/5.8.3-email-verification/","title":"Setup Email Verification","tags":[],"description":"","content":"Setup Email Verification In this step, you will configure email verification to ensure users verify their email addresses during registration.\nNavigate to your User Pool in the Cognito console Go to Sign-up experience tab Click Edit in the Attribute verification and user account confirmation section Configure Email Verification Attribute verification and user account confirmation:\n✅ Send email message, verify email address Verification message: Code Email verification subject: Verify your email for Task Management System Email verification message: Your verification code for Task Management System is {####}. Please enter this code to complete your registration. Configure Email Delivery Email delivery method:\nSend email with Cognito (for development) FROM email address: no-reply@verificationemail.com Note: For production applications, consider using Amazon SES for better email deliverability and custom domains.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Purpose of the Workshop Provide a comprehensive overview of DevOps culture, principles, and practices on AWS. Demonstrate how to build a complete CI/CD pipeline—from source control to automated deployment. Introduce major Infrastructure as Code (IaC) tools on AWS and their practical applications. Equip attendees with knowledge on containerization, observability, and operational best practices. Speakers / Main Instructors (Expected) Đỗ Huy Thắng – DevOps Lead, VNG Nguyễn Thị Thu Hà – Senior DevOps Engineer, AWS Phạm Tuấn Anh – Solutions Architect, AWS Key Topics \u0026amp; Learnings 1. DevOps Culture \u0026amp; Core Principles DevOps is not just about tools—it represents a collaborative culture between Development and Operations. DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) are critical indicators of DevOps performance. Shift-left Testing \u0026amp; Security helps detect issues earlier and reduce remediation cost. 2. CI/CD Pipeline on AWS AWS DevTools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) enable fully automated release workflows. Deployment strategies include Blue/Green, Canary, and Rolling Updates. Live demonstration: commit → build → test → deploy to EC2/ECS. 3. Infrastructure as Code (IaC) AWS CloudFormation: declarative IaC using YAML/JSON templates. AWS CDK: imperative IaC using languages like Python or TypeScript, offering strong reusability. Drift Detection ensures infrastructure remains consistent with defined configurations. 4. Containers \u0026amp; Microservices Docker packages applications into lightweight, portable, isolated containers. Comparison of Amazon ECS vs Amazon EKS to choose the right orchestration tool. AWS App Runner offers serverless container deployment with minimal operational overhead. 5. Monitoring \u0026amp; Observability Amazon CloudWatch: metrics, logs, alarms, dashboards for real-time system health. AWS X-Ray: distributed tracing to analyze performance and identify latency bottlenecks. Best practices include meaningful alerts, dashboards aligned with SLOs, and a structured postmortem process. Key Takeaways Mindset \u0026amp; Culture DevOps is a continuous improvement journey. Automation is the backbone of DevOps. Adopt a “blameless postmortem” mindset to learn from failures. Technical Knowledge CI/CD is about creating a reliable delivery workflow, not just running scripts. IaC enables consistency, version control, and environment reproducibility. Observability provides deeper insights into root cause analysis. Career Skills DevOps engineers need broad knowledge: networking, security, coding, system operations. Advanced debugging skills are essential for distributed systems. Personal Action Plan Build a personal CI/CD pipeline using CodeCommit \u0026amp; CodePipeline. Practice AWS CDK using Python to deploy core AWS resources. Package a small application into Docker and deploy it on ECS Fargate. Create a CloudWatch dashboard with essential metrics and alarms. Personal Reflection This workshop offered highly practical insights that developers often overlook.\nThe CI/CD and IaC demonstrations were particularly impressive, showing how automation reduces risk and accelerates release cycles.\nReal-world case studies from VNG and AWS emphasized the critical role of monitoring and DevOps culture.\nPhotos "},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 - 17:00 , September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 - 16:30, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:00 - 11:3, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 8:30 - 17:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 8:30 - 12:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn how to deploy VPC Peering between multiple VPCs. Learn how to configure Transit Gateway for large-scale inter-VPC connectivity. Practice advanced labs related to routing and DNS across VPCs. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the fundamentals of VPC Peering: how it works, limitations, routing behavior - Learn about the concept of peering inside a VPC 29/09/2025 29/09/2025 https://000019.awsstudygroup.com 3 - Hands-on: + Create EC2 instances in multiple VPCs and configure routing + Verify NACLs and Security Groups when using peering + Finalize the peering setup 30/09/2025 30/09/2025 https://000019.awsstudygroup.com 4 - Learn about Transit Gateway - Hands-on: + Deploy a TGW + Set up routing between VPCs and the TGW 01/10/2025 01/10/2025 https://000020.awsstudygroup.com 5 - Hands-on: + Create a Transit Gateway connection + Create a TGW Route Table + Map VPC routes to the TGW + Final connectivity check between multiple VPCs via TGW 02/10/2025 02/10/2025 https://000020.awsstudygroup.com - Clean up all lab resources - Consolidate knowledge of Peering and Transit Gateway 02/10/2025 02/10/2025 https://000020.awsstudygroup.com 6 03/10/2025 03/10/2025 Week 4 Achievements: Summary:\nDuring this week, I explored different models for connecting multiple VPCs, including VPC Peering and Transit Gateway. I gained a clear understanding of how inter-VPC networking works, how routing is performed through TGW, and how each component functions within a multi-VPC architecture. Theory learned:\nHow VPC Peering operates and how routing is handled between two VPCs. The architecture and capabilities of Transit Gateway (TGW) for connecting multiple VPCs in a hub-and-spoke topology How to configure route tables in VPCs connected via Peering or Transit Gateway. Hands-on labs:\nCreated VPC Peering connections and updated route tables to enable traffic between VPCs. Configured a Transit Gateway, created TGW attachments, and set up routing between connected networks. *Tested connectivity across subnets and VPCs to validate the configuration. "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.4-apigateway/","title":"API Gateway","tags":[],"description":"","content":"Initializing API Gateway Create an API Gateway Function Open API Gateway\nSelect Create API Choose API type: REST API\nThen select: Build\nConfigure API Information Enter API name, for example: taskhub-backend-api\nEndpoint type: Regional\nSecurity policy: SecurityPolicy_TLS13_1_3_2025_09\nClick Create API\nCreate Resource for API In the API Gateway menu, select Resources\nClick Create resource\nEnter:\nResource name: auth, task, projects … depending on the project Resource path: /auth, /task, … Click Create resource\nCreate Method and Connect to Lambda Select a Resource → click Create method\nChoose ANY (or POST, GET depending on your API) Integration type: Lambda Function Tick Use Lambda Proxy integration Select Region Enter the name of the Lambda function, e.g., taskhub-backend_1 Click Save\nRepeat these steps to create additional API routes.\nGrant API Gateway Permission to Call Lambda Choose Deploy API\nCreate a new stage (if not available): prod1 Click Deploy Result: You will receive an Invoke URL like:\nhttps://ne6pw5hqej.execute-api.ap-southeast-1.amazonaws.com/prod1\n"},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.8-cognito/5.8.4-app-client/","title":"Configure App Client","tags":[],"description":"","content":"Configure App Client When creating the User Pool, AWS Cognito automatically created a default App Client. In this step, we will configure this App Client to suit our application needs.\nNavigate to your User Pool in the Cognito console Go to App integration tab You will see the App Client has already been created Edit App Client Click on the App Client name to edit it Or click Edit if available Update app client information:\nApp client name: TaskManagementWebApp (if you want to change it) App client type: Public client Authentication flows: ✅ ALLOW_USER_PASSWORD_AUTH ✅ ALLOW_REFRESH_TOKEN_AUTH ✅ ALLOW_USER_SRP_AUTH Configure Hosted UI Hosted UI settings:\nUse the Cognito Hosted UI: Enabled Domain type: Use a Cognito domain Cognito domain: taskmanagement-auth-[your-unique-id] Initial app client settings:\nAllowed callback URLs: http://localhost:3000/callback https://your-app-domain.com/callback Allowed sign-out URLs: http://localhost:3000/ https://your-app-domain.com/ OAuth 2.0 settings:\nAllowed OAuth flows: ✅ Authorization code grant ✅ Implicit grant Allowed OAuth scopes: ✅ email ✅ openid ✅ profile Save Configuration Review all configurations Click Save changes Verify App Client Configuration After updating, note down the important information:\nApp Client Details:\nClient ID: [your-client-id] Hosted UI URL: https://taskmanagement-auth-[your-id].auth.us-east-1.amazoncognito.com The App Client is now configured and ready to handle authentication requests from your application. You can use the Client ID and Hosted UI URL to integrate with your web or mobile application.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Task Management Platform with DevOps on AWS Serverless Overview AWS Serverless enables you to build and deploy applications without managing servers, automatically scales based on demand, and you only pay for what you use.\nIn this workshop, we will learn how to design, build, and deploy a complete task management platform TaskHub using serverless architecture and automated DevSecOps practices.\nWe will create a system that includes frontend, backend API, database, and a complete CI/CD pipeline. The workshop focuses on three main components to build a production-ready application on AWS:\nServerless Backend - Use AWS Lambda for business logic processing, API Gateway as the communication layer, DynamoDB for data storage, and Cognito for user authentication management with optimized costs.\nContent Delivery - Deploy Next.js application on S3, distribute globally via CloudFront with low latency, and protect with AWS WAF against common web attacks.\nDevOps Pipeline - Automate the build, test, and deploy process using CodePipeline and CodeBuild, integrate security scanning with CodeGuru, and manage infrastructure as code with CloudFormation.\nContent Workshop overview Prerequiste Deploying Serverless Functions with AWS Lambda Building an API Gateway with Amazon API Gateway Simple and Secure Object Storage with Amazon S3 Accelerating Content Delivery with Amazon CloudFront (CDN) Managing User Identity and Access with Amazon Cognito Managing Encryption Keys with AWS Key Management Service (KMS) SecretManager WAF "},{"uri":"https://tuananh3232.github.io/aws_intership/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3: AWS Well-Architected – Security Pillar\u0026rdquo; Purpose of the Workshop Introduce the AWS Well-Architected Framework with a focus on the Security Pillar, the foundation of all AWS architectures. Provide practical knowledge on five key security domains: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Build a “Security by Design” mindset with core models such as Least Privilege, Zero Trust, and Defense in Depth. Share real-world best practices and lessons learned from enterprises in Vietnam. Speakers / Instructors (Expected) Nguyễn Văn Tú – Sr. Security Specialist Solutions Architect, AWS Lê Thị Thanh Mai – Cloud Security Engineer, AWS Partner Đặng Quốc Bảo – Security Lead, Fintech Company (Guest case study) Key Content \u0026amp; Learnings 1. Foundational Security Mindset Security is the first pillar in every AWS architectural design. Core principles: Least Privilege – minimal permissions assignment. Zero Trust – never trust, always verify. Defense in Depth – multiple layers of protection. The Shared Responsibility Model defines responsibilities between AWS and customers—understanding this boundary is critical. 2. Pillar 1 – Identity \u0026amp; Access Management (IAM) Avoid long-term credentials; use IAM Roles for EC2, Lambda, and ECS. IAM Identity Center \u0026amp; SSO provide centralized access management. SCPs and Permission Boundaries enforce organization-wide guardrails. IAM Access Analyzer helps detect unintended public access. 3. Pillar 2 – Detection \u0026amp; Continuous Monitoring Enable comprehensive logging: Organization CloudTrail, VPC Flow Logs, ALB Logs. GuardDuty detects suspicious activity; Security Hub aggregates findings. Detection-as-Code using AWS Config Rules ensures consistent policies. Automate alerts using Amazon EventBridge. 4. Pillar 3 – Infrastructure Protection Secure network design using VPC segmentation (Public, Private, Data subnets). Understand Security Groups (stateful) vs NACLs (stateless). Protect applications using AWS WAF and AWS Shield. Implement OS hardening and IMDSv2 for EC2 instances. 5. Pillar 4 – Data Protection Encryption is the default: KMS for S3, RDS, EBS, DynamoDB. Manage keys \u0026amp; secrets with KMS Key Policies and Secrets Manager. Classify data sensitivity and assign tags for appropriate access control. 6. Pillar 5 – Incident Response Prepare playbooks for common security scenarios. Standard IR workflow: Contain → Eradicate → Recover → Analyze. Automate security actions using Lambda and Step Functions. Key Takeaways Strategy \u0026amp; Mindset Security is job zero—security must be designed upfront. Apply risk-based prioritization to security efforts. Continuously measure and improve using metrics and Security Hub scores. Technical Lessons Reduce attack surface by minimizing public resources. Encrypt everything by default. Automate security operations to minimize human error. Personal Action Plan Review IAM policies and enforce Least Privilege. Enable GuardDuty and Security Hub on personal AWS accounts. Create a KMS key and encrypt an S3 bucket as practice. Study the Shared Responsibility Model deeper for services in current use. Personal Reflection This workshop reshaped my understanding of cloud security.\nSecurity is no longer a mysterious topic—it becomes a structured, logical framework when broken into the five pillars.\nThe demo of IAM Policy Simulator and real Incident Response scenarios showed how modern security relies on strict policy enforcement combined with automated workflows.\nThe repeated reminder that “Security is everyone’s job” left a strong impact.\nA misconfigured Security Group or a poorly validated piece of code can expose an entire system.\nThe workshop ended with a stronger sense of responsibility, motivating me to apply secure practices even in the smallest projects.\nPhotos "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn the core components of EC2 and how compute operates in AWS. Understand Auto Scaling, EBS, Instance Store, User Data, Metadata. Practice backup, Storage Gateway, and deploying EC2 for storage-related use cases. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn about EC2, instance types, AMI, key pair - Understand EBS, Instance Store, User Data, Metadata 06/10/2025 06/10/2025 https://youtu.be/-t5h4N6vfBs?si=GeVdhO9IEDjzzS_D https://youtu.be/e7XeKdOVq40?si=T3I4pgPoEfVytcU3 https://youtu.be/yAR6QRT3N1k?si=GQghyBwLCpijrDON https://youtu.be/hKr_TfGP7NY?si=gR2MqaLAFrqL-KBo https://youtu.be/6IHNDJ85aoQ?si=M0puk6DJpliO7ahf https://youtu.be/_v_43Wi7zjo?si=qNDVWzKcQFNO2mGh https://youtu.be/Ew3QRaKJQSA?si=xNvXvD8yFhnSMJby 3 - Understand EC2 Auto Scaling and how VM scaling works - Learn about storage and compute services (EFS/FSx, Lightsail, MGN overview) 07/10/2025 07/10/2025 https://youtu.be/bbLcPitXJSY?si=eyVnxvL9ho0LpUYy https://youtu.be/hFVYG8WqfU0?si=9Px4wmR4IRZxk15n 4 - Hands-on: + Deploy AWS Backup + Create backup plan + Test restore \u0026amp; cleanup + Clean up backup 08/10/2025 08/10/2025 https://000013.awsstudygroup.com 5 - Hands-on: + Create an S3 bucket for Storage Gateway + Create EC2 for Storage Gateway + Create Storage Gateway + File Share + Clean up Storage Gateway 09/10/2025 09/10/2025 https://000024.awsstudygroup.com 6 - Hands-on: + Create bucket, upload data + Enable static website hosting + Configure public access block + Configure CloudFront and test website + Clean up website + CloudFront + bucket 10/10/2025 10/10/2025 https://000057.awsstudygroup.com Week 5 Achievements: Summary:\nThis week I learned how EC2 operates, different instance storage types, Auto Scaling, and backup mechanisms. I also practiced Storage Gateway and deployed an S3 static website. Theory learned:\nEC2 architecture, AMI, key pair EBS vs Instance Store User Data / Metadata EC2 Auto Scaling Storage Gateway and fundamentals of AWS Backup Hands-on labs:\nCreate backup plan + test restore Create Storage Gateway + file share Deploy static website using S3 + CloudFront "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.6-s3/","title":"Set up S3","tags":[],"description":"","content":"WORKLOG: S3 ORIGIN CONFIGURATION (FULL) This Worklog outlines the steps for creating and configuring two (02) separate Amazon S3 Buckets, serving as the Origins for different system resources: Frontend Code and user-uploaded files.\n1. S3 Bucket 1 Configuration: taskhub-frontend-prod (Frontend Origin) This Bucket serves as the Origin for CloudFront, storing Frontend code (HTML, CSS, JS) and static assets.\n1.1. Access and Bucket Creation Log in to the AWS Console, find and select the Amazon S3 service. Click the \u0026ldquo;Create bucket\u0026rdquo; button. Fill in the general configuration information: Configuration Value Explanation Bucket name taskhub-frontend-prod The Bucket name must be globally unique. AWS Region Asia Pacific (Singapore) ap-southeast-1 Select the geographical region closest to the target users to optimize latency. 1.2. Object Ownership \u0026amp; Public Access Configuration Object Ownership: Select ACLs disabled (recommended). Purpose: Access permissions are managed centrally using a Bucket Policy, simplifying management. Block Public Access settings for this bucket: Action: Uncheck [ ] Block all public access (and all sub-options). Reason: This allows us to configure a Bucket Policy later to grant read-only access specifically to CloudFront OAC (Origin Access Control), ensuring S3 can function as a valid private Origin. 1.3. Versioning and Encryption Configuration Configuration Value Explanation Bucket Versioning Select: Disable Reduces Storage Costs as maintaining multiple versions of Frontend code is unnecessary. Default encryption Enable Ensures data is encrypted when stored (at rest). Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) The default, cost-effective encryption method. Bucket Key Select: Enable Minimizes Request Costs related to the encryption/decryption process. 2. S3 Bucket 2 Configuration: taskhub-files-prod (User Files Storage) This Bucket is used to store files uploaded by users (images, media\u0026hellip;). Maximum security is prioritized. The creation process is similar to the S3 Bucket above.\n2.1. Access and Bucket Creation Repeat the Bucket creation steps (Section 1.1). Bucket name: taskhub-files-prod AWS Region: Asia Pacific (Singapore) ap-southeast-1 2.2. Public Access Configuration (SECURITY DIFFERENCE) Block Public Access settings for this bucket: Keep [X] Block all public access (CHECK ALL). Reason: This is a Secure Bucket containing user data. Files MUST NOT be publicly accessible. Access will only be temporarily granted via Pre-signed URLs generated by the Backend API after authentication. 2.3. Versioning and Encryption Configuration (Similar) Configuration Value Explanation Bucket Versioning Select: Disable Prevents rapid storage cost increases when users update/delete files. Default encryption Enable Data encryption is mandatory for user data. Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) Standard S3 encryption. Bucket Key Select: Enable Reduces encryption request costs. "},{"uri":"https://tuananh3232.github.io/aws_intership/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Company Limited from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI participated in hands-on training and specialized workshops on AWS Cloud, through which I strengthened my skills in working with cloud services, technology analysis, technical reporting, and communication in a professional environment.\nRegarding work ethic, I consistently strived to complete assigned tasks, complied with company policies, and actively communicated with colleagues to improve work efficiency.\nTo objectively reflect on my internship experience, I provide the following self-evaluation:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ☐ ✅ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ☐ ✅ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Strengths Quick learner with the ability to acquire new knowledge efficiently. Strong adherence to work principles and guidelines. Responsible in completing assigned tasks. Friendly, cooperative, and supportive in team environments. Areas for Improvement Communication confidence: Need to be more proactive and confident in communication. Deep problem exploration: Improve the ability to analyze issues deeply instead of focusing only on surface-level solutions. Development Plan Create a detailed study and practice schedule to improve learning efficiency. Actively participate in discussions to enhance communication confidence and presentation skills. "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn the overview of AWS storage services (S3, Glacier, Backup, Storage Gateway, Snow Family). Understand how S3 works: access point, storage class, CORS, static website hosting. Practice the entire workflow with S3, Backup, Storage Gateway, and File Systems. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the overview of AWS storage services: S3, EBS, Backup, Storage Gateway, Snow Family - Study Access Point, Storage Class, and data access models - Understand S3 static website, CORS, Object key, Glacier 13/10/2025 13/10/2025 https://youtu.be/hsCfP0IxoaM?si=O3vMWs7Trr1fugJD https://youtu.be/_yunukwcAwc?si=ZhkTKr-_OkyUNImI https://youtu.be/mPBjB6Ltl_Q?si=qs6j0n7AeD2Mxwbz https://youtu.be/YXn8Q_Hpsu4?si=XojTnkR_LLC1KwEv 3 Hands-on: + Create S3 bucket + Deploy backup infrastructure + Create backup plan and set notification + Test restore \u0026amp; clean up backup resources 14/10/2025 14/10/2025 https://000013.awsstudygroup.com 4 - Learn VMware Workstation - Hands-on: + Export VM from on-prem + Upload VM to AWS + Import as EC2 + Export back as AMI + Clean up import/export environment 15/10/2025 15/10/2025 https://000014.awsstudygroup.com 5 - Hands-on: + Create Storage Gateway + Create advanced File Share + Connect File Share from on-prem machine + Clean up Storage Gateway + File Shares 16/10/2025 16/10/2025 https://000024.awsstudygroup.com 6 - Hands-on (lab25): + Create FSx file system (SSD/HDD, Multi-AZ) + Create \u0026amp; configure file shares + Test \u0026amp; monitor performance + Manage user sessions + quotas - Hands-on (lab57): + Create bucket, upload data, enable static website + Configure public access + object permissions + Create \u0026amp; configure CloudFront distribution + Enable versioning \u0026amp; object replication - Clean up environment (lab25), bucket, CloudFront, replication 17/10/2025 17/10/2025 https://000025.awsstudygroup.com https://000057.awsstudygroup.com Week 6 Achievements: Summary:\nThis week, I gained a solid understanding of the AWS storage ecosystem, including S3, Glacier, AWS Backup, Storage Gateway, and various file systems. I focused heavily on hands-on practice to understand how data management, backup–restore workflows, and storage mechanisms operate in AWS. Theory learned:\nConcepts of S3 Storage Classes, Access Points, and CORS. Knowledge about Glacier, lifecycle policies, and AWS backup concepts. Architecture and operation of AWS Storage Gateway and AWS file system services. Processes for importing/exporting virtual machines to AWS. Hands-on labs:\nBackup \u0026amp; restore Import on-prem VM into AWS Create Multi-AZ file system Built a static website using S3, integrated CloudFront, enabled versioning and replication. Worked with Storage Gateway: created a file share, connected from on-prem, and tested data transfer between on-prem and AWS. "},{"uri":"https://tuananh3232.github.io/aws_intership/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment at AWS through the FCJ program is highly professional and well-organized. The learning and working space is structured effectively, enabling me to focus and absorb knowledge efficiently. FCJ regularly organizes workshops, which help me expand my understanding and stay updated with the latest technologies available on AWS.\n2. Support from Mentor / Team Admin\nMy mentor provided detailed guidance, explained clearly whenever I did not fully understand, and consistently encouraged me to ask questions. I truly appreciate that the mentor let me try and solve problems on my own instead of giving me the answer immediately, which helped me become more proactive and improve quickly. The admin team was also very supportive, preparing all necessary documents and procedures to ensure a smooth working process. Whenever I had questions, both the mentor and admin team responded quickly and provided thorough explanations, motivating me throughout the internship.\n3. Relevance of Work to My Academic Major\nDuring the internship, I learned many important skills—from technical knowledge such as working with AWS services to soft skills like teamwork, presentation, and report writing. The sharing sessions from experts and FCJ members gave me new perspectives and valuable insights for shaping my future career development.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship, I gained a wide range of new skills, including the use of project management tools, effective teamwork, and professional communication in a corporate setting. My mentor also shared a lot of practical experience that helped me build clearer career directions, especially in the DevOps and cloud domain.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously, yet maintains a friendly and enjoyable atmosphere. When urgent projects arise, everyone collaborates and supports one another regardless of their position or role. This helped me feel like a true part of the team, even though I was only an intern.\n6. Internship Policies / Benefits\nThe company offers an internship allowance and provides flexible working hours when necessary. Additionally, the opportunity to participate in internal training sessions, workshops, and knowledge-sharing activities is a major advantage that supports continuous learning.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with the opportunity to work directly with DevOps and AWS tasks. Being able to deploy infrastructure, configure services, and troubleshoot real issues helped me improve quickly. I also learned a great deal about DevSecOps practices, including IAM management, CI/CD pipelines, and system operations.\nWhat do you think the company should improve for future interns?\nI think the program could include additional advanced DevOps workshops, such as building complete CI/CD pipelines, running incident simulation exercises, or conducting mini-projects in teams. This would help interns gain a deeper understanding of real-world system operations.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, definitely! The program provides highly practical knowledge in DevOps and AWS, with hands-on experience rather than just theory. The mentors are supportive, the environment is professional yet friendly, and it is especially suitable for students pursuing a Cloud/DevOps career path.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest adding more group practical exercises followed by a final consolidated project. This would help interns strengthen teamwork and apply knowledge in a more integrated manner.\nWould you like to continue this program in the future?\nIf given the opportunity, I would love to continue the program. I hope to take part in more advanced topics such as building complete DevOps systems, multi-environment CI/CD pipelines, and AWS cost optimization.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn the complete IAM system: user, group, role, policy, permission boundary. Understand AWS authentication \u0026amp; authorization mechanisms, how to write JSON policies, and how policy evaluation works. Get familiar with AWS Organizations, Organizational Units (OU), and Service Control Policies (SCP). Practice IAM + Organization labs to understand how large-scale account management works. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the IAM fundamentals: user, group, role, policy - Understand policy evaluation: explicit deny, implicit deny, allow 20/10/2025 20/10/2025 https://youtu.be/tsobAlSg19g?si=9f3mlIWPtrCcNuKg https://youtu.be/N_vlJGAqZxo?si=e8oiWCObco95CoKh 3 - Hands-on: + Create user/group/role + Attach inline \u0026amp; managed policies + Test S3/EC2 access under different policies - Learn permission boundaries and session policies 21/10/2025 21/10/2025 https://000028.awsstudygroup.com 4 - Study AWS Organizations: OU structure, multi-account setup - Understand SCP concepts, deny list vs allow list 22/10/2025 22/10/2025 https://youtu.be/5oQY8Rogz9Y?si=h8DlUb8ZLI4HbbvM https://youtu.be/NW1xrMkNMjU?si=dhT0T3y2JYVK8QwT 5 - Hands-on: + Create Organization + OU + Apply SCP deny EC2 / deny S3 + Verify SCP effectiveness combined with IAM policies + Rearrange OU, remove SCP 23/10/2025 23/10/2025 https://000030.awsstudygroup.com https://000044.awsstudygroup.com/ 6 - Team activity: + Discuss workshop ideas + Plan execution + Divide tasks for workshop 24/10/2025 24/10/2025 Week 7 Achievements: Summary:\nThis week I learned the foundation of AWS access management, including IAM and Organizations. I now understand how policy evaluation works, how to write JSON policies, and how SCPs apply across multi-account environments. Theory learned:\nConcepts of User – Group – Role – Policy and how evaluation works Inline policy, managed policy, permission boundary AWS Organizations structure, OU SCP concepts and differences compared to IAM policies Landing Zone \u0026amp; Control Tower overview Hands-on labs:\nCreate user/group/role and test different access levels Write JSON policies and test allow/deny behaviors Create Organization, OU, and apply SCP Validate SCP + IAM policy combinations in practice "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.7-cloudfront/","title":"Set up CloudFront","tags":[],"description":"","content":"COMPLETE CLOUDFRONT DISTRIBUTION CONFIGURATION This Worklog outlines the steps for configuring a CloudFront Distribution, focusing on setting up Origin Access Control (OAC) security and verifying the prerequisites on the S3 Bucket.\n1. S3 PREREQUISITES CHECK Before finalizing CloudFront setup, you must ensure the S3 Bucket taskhub-frontend-prod is protected and configured correctly.\n1.1. Static Website Hosting Status Check: The Properties tab of the S3 Bucket. Status: S3 static website hosting must be in the Disabled state. Explanation: Since CloudFront acts as the CDN, Static Website Hosting is not required on S3. S3 merely serves as the content repository (Origin). 1.2. Block Public Access \u0026amp; Bucket Policy Check: The Permissions tab of the S3 Bucket. Block public access (bucket settings): Must be in the On state (Block all public access). Bucket policy (Final Confirmation): Must contain the OAC Policy that is automatically updated by CloudFront . 2. CLOUDFRONT DISTRIBUTION CONFIGURATION 2.1. Step 1 \u0026amp; 2: Get started Configuration Value Explanation Action Click \u0026ldquo;Create distribution\u0026rdquo;. Start the CDN creation process. Plan Select Free Plan ($0/month). The free plan for the project. Distribution name taskhub-frontend-cdn A memorable name for the resource. Distribution type Single website or app. The appropriate type for a Frontend application. 2.2. Step 3: Specify origin (OAC Setup) 1. Specify Origin Origin type: Select Amazon S3. S3 origin: Select the S3 Bucket taskhub-frontend-prod. 2. Configure OAC (Automatic Security) Tick \u0026ldquo;Allow private S3 bucket access to CloudFront - Recommended\u0026rdquo;. Select Use recommended origin settings. Explanation: Upon completing the Distribution creation, CloudFront will automatically update the S3 Bucket Policy to grant access via OAC. 3. Cache Configuration Cache settings: Select Use recommended cache settings tailored to serving S3 content. Default Root Object: Enter index.html (Standard configuration for SPA). 2.3. Step 4 \u0026amp; 5: Security \u0026amp; TLS Configuration Applied Value Explanation WAF Uncheck paid features. Keeping defaults for the project. Viewer protocol policy Redirect HTTP to HTTPS Mandates the use of the secure protocol. Custom SSL certificate Default CloudFront Certificate Activates free HTTPS. 2.4. Step 6: Review and create 1. Review Origin (Source Confirmation) S3 origin: taskhub-frontend-prod. (Must ensure the correct Frontend Bucket is selected). Grant CloudFront access to origin: Must be Yes. (Confirms OAC functionality). 2. Final Configuration Price Class: Select Use all edge locations (best performance). Click the \u0026ldquo;Create distribution\u0026rdquo; button to deploy. 3. POST-DEPLOYMENT CHECK: OAC SECURITY CONFIRMATION After the Distribution status changes to Deploying, perform this crucial check to confirm that OAC is functional:\nAccess S3 Bucket taskhub-frontend-prod. Navigate to the \u0026ldquo;Permissions\u0026rdquo; tab. Find the \u0026ldquo;Bucket policy\u0026rdquo; section. Confirm: The Policy must be automatically updated and contain the JSON statement authorizing the CloudFront service. This confirms that OAC has blocked direct access and only allows CloudFront to read the files. "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn about database systems on AWS: RDS, Aurora, Redshift, ElastiCache. Practice building a database subnet group, test connectivity, backup \u0026amp; restore. Learn data analytics services such as Kinesis, Glue, Athena, QuickSight. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Learn about Databases: RDS, Aurora, Redshift, ElastiCache - Learn about Multi-AZ architecture, read replicas, backup/restore 27/10/2025 27/10/2025 https://youtu.be/OOD2RwWuLRw?si=9JsOs0PNfO1TdAUl https://youtu.be/qbrobQZrokY?si=ePJjzYXWg3qE_Ca6 https://youtu.be/UvdiRW34aNI?si=8g3FwgsJ3VLT-_nf 3 - Practice: + Create VPC + SG for EC2 + RDS + Create DB subnet group + Deploy EC2 + Create RDS instance + Backup \u0026amp; Restore 28/10/2025 28/10/2025 https://000005.awsstudygroup.com 4 - Practice: + Connect to MSSQL/Oracle + Schema Conversion + Create DMS Task + Inspect logs, troubleshoot 29/10/2025 29/10/2025 https://000043.awsstudygroup.com 5 - Learn about Data Analytics (Kinesis, Glue, Athena, QuickSight) - Practice: + Create DynamoDB table + Enable autoscaling + CRUD test + Create Global Table and clean up resources 30/10/2025 30/10/2025 https://000039.awsstudygroup.com 6 - Practice (lab35): + Create S3 bucket + Create Kinesis Firehose ingestion + Glue crawler + Query data with Athena + Create QuickSight dashboard - Practice (lab40): + Check cost allocation + Tagging resources + Additional queries \u0026amp; resource cleanup 31/10/2025 31/10/2025 https://000035.awsstudygroup.com https://000040.awsstudygroup.com Week 8 Achievements: Overview:\nThis week, I focused on AWS database and data analytics services, including RDS, Aurora, DynamoDB, DMS, Kinesis, Glue, Athena, and QuickSight. I gained a solid understanding of database architecture, connectivity, backup/restore, autoscaling, as well as the end-to-end data analytics pipeline from ingestion -\u0026gt; ETL -\u0026gt; query -\u0026gt; visualization. Theory Learned:\nConcepts of RDS, Aurora architecture, Multi-AZ, read replicas Backup, snapshot, parameter group, option group DynamoDB: partition key, sort key, throughput, autoscaling, DAX Overview of Data Analytics: Kinesis Firehose, Glue crawler, Athena, QuickSight Database Migration concepts: schema conversion, DMS task Lab Practice:\nCreated VPC + security groups for EC2/RDS Created DB subnet group, deployed EC2 and RDS MySQL Performed Backup \u0026amp; Restore Connected to MSSQL/Oracle, practiced Schema Conversion \u0026amp; created DMS task Created DynamoDB table, enabled autoscaling, CRUD test, created Global Table \u0026amp; cleanup Built analytics pipeline: Kinesis Firehose -\u0026gt; S3, Glue crawler, Athena queries, and QuickSight dashboard Performed additional tasks: tagging, cost allocation, and resource cleanup "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.8-cognito/","title":"Setup AWS Cognito Authentication","tags":[],"description":"","content":"AWS Cognito User Authentication Workshop Overview AWS Cognito provides user identity and access management for web and mobile applications. It enables you to add user sign-up, sign-in, and access control to your applications quickly and easily.\nIn this workshop, you will learn how to:\nCreate and configure a Cognito User Pool Setup password policies and email verification Configure App Client with Hosted UI Implement basic email/password authentication Content Create Cognito User Pool Configure Password Policies Setup Email Verification Configure App Client "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.9-keymanagementservice/","title":"Set up KMS","tags":[],"description":"","content":"Objective Create a Customer Managed Key (CMK) on AWS KMS to:\nEncrypt data in DynamoDB Encrypt Secrets Manager Ensure DevSecOps standard – data is encrypted using KMS Step 1 – Access AWS Key Management Service (KMS) Log in to AWS Console Search for service: KMS Select Key Management Service Go to menu Customer managed keys Click Create a key Step 2 – Configure Key At Key type, select Symmetric At Key usage, select Encrypt and decrypt Keep other options as default Click Next Step 3 – Add Labels (Alias \u0026amp; Description) Alias Enter:\ntaskhub_kms Click Next\nStep 4 – Define Key Administrative Permissions In the Key administrators list, select QuocBao Keep the option Allow key administrators to delete this key enabled Click Next The selected user at this step has full administrative permissions for the KMS Key:\nEdit key policy Enable / disable the key Delete the key Step 5 – Define Key Usage Permissions In the Key users list, select QuocBao No need to add Other AWS accounts Click Next Step 6 – Edit Key Policy At the Edit key policy step, click Edit Verify that the policy includes the following permission groups: Root account: kms:* User QuocBao is allowed to manage and use the key { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;key-consolepolicy-3\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow access for Key Administrators\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Create*\u0026#34;, \u0026#34;kms:Describe*\u0026#34;, \u0026#34;kms:Enable*\u0026#34;, \u0026#34;kms:List*\u0026#34;, \u0026#34;kms:Put*\u0026#34;, \u0026#34;kms:Update*\u0026#34;, \u0026#34;kms:Revoke*\u0026#34;, \u0026#34;kms:Disable*\u0026#34;, \u0026#34;kms:Get*\u0026#34;, \u0026#34;kms:Delete*\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:RotateKeyOnDemand\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow use of the key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow attachment of persistent resources\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:ListGrants\u0026#34;, \u0026#34;kms:RevokeGrant\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Bool\u0026#34;: { \u0026#34;kms:GrantIsForAWSResource\u0026#34;: \u0026#34;true\u0026#34; } } } ] } Modify the policy if necessary to match your actual account ARN Click Next AWS will automatically attach a valid policy to the key.\nStep 7 – Review \u0026amp; Finish Review all configurations: Item Value Key type Symmetric Key usage Encrypt and decrypt Alias taskhub_kms Key Admin QuocBao Key User QuocBao Click Finish AWS will start creating the KMS Key.\nStep 8 – Verify KMS Key Created Successfully After creation, go back to KMS → Customer managed keys Verify the following information: Alias: taskhub_kms Status: Enabled Key type: Symmetric Key spec: SYMMETRIC_DEFAULT Key usage: Encrypt and decrypt The KMS Key has now been successfully created.\n"},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives Understand the AWS Serverless architecture and identify the infrastructure components that need to be managed. Design the application deployment architecture (Backend, Frontend, Auth, Edge Security) from a DevOps perspective. Research and design the security layer: CloudFront, Route 53, AWS WAF, and API protection. Study Amazon Cognito and its integration into the authentication/authorization pipeline. Understand the CI/CD workflow using CodePipeline, CodeBuild, GitLab Runner, and Infrastructure-as-Code (CloudFormation). Set up monitoring and observability mechanisms: CloudWatch Logs/Metrics, AWS X-Ray, SNS Alerts. Tasks to Be Completed This Week Day Task Start Date End Date Notes 2 - Analyze Serverless architecture 03/11/2025 03/11/2025 - Study how API Gateway – Lambda – DynamoDB operate from a DevOps perspective 03/11/2025 03/11/2025 3 - Design the overall architecture (Infrastructure Level) 03/11/2025 03/11/2025 - Define deployment flow and execution environments 04/11/2025 04/11/2025 4 - Design the Edge Security layer (CloudFront + WAF + Route 53) 05/11/2025 05/11/2025 5 - Study Cognito Tokens and authentication flow 06/11/2025 06/11/2025 6 - Design the CI/CD Pipeline (GitLab → AWS) 07/11/2025 07/11/2025 Week 9 Achievements Overview This week focused on analyzing and designing the Serverless infrastructure to ensure all application components can be properly managed, monitored, and automatically deployed. I gained a deep understanding of how API Gateway, Lambda, and DynamoDB interact from a DevOps standpoint, including scaling behavior, logging, retry mechanisms, throttling, security, and the CI/CD pipeline.\nTopics Learned Event-driven Serverless model and automatic autoscaling. DevOps responsibilities: logging, tracing, IAM structuring, and security. Managing versions, deployments, and stage variables in API Gateway and Lambda. Security architecture using CloudFront → WAF → API Gateway. CI/CD automation with CodePipeline, CodeBuild, and CloudFormation. Hands-on Work / Deliverables Designed a complete Serverless infrastructure architecture diagram. Built a request-flow diagram from FE → CloudFront → WAF → API Gateway → Lambda → DynamoDB. Created deployment pipeline designs: GitLab CI/CD → AWS CLI → Lambda GitLab → S3 + CloudFront Designed the DevOps authentication flow: Cognito → Authorizer → API Gateway. "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives Validate and finalize the local development environment for Backend and DevOps workflows (.NET SDK, Docker Desktop, NoSQL Workbench). Set up DynamoDB Local using Docker and validate the data model using NoSQL Workbench. Perform CRUD testing on DynamoDB Local and confirm access patterns before deploying to AWS. Collaborate with the Frontend team to connect APIs and ensure end-to-end functionality in the local environment. Prepare a stable technical foundation before Week 11 (migrating Backend → Lambda \u0026amp; API Gateway). Tasks to Be Completed This Week Day Task Start Date End Date 2 - Re-validate Backend environment: .NET SDK, Docker, NoSQL Workbench - Review system architecture 10/11/2025 10/11/2025 3 - Set up DynamoDB Local via Docker - Connect with NoSQL Workbench and test manual CRUD 11/11/2025 11/11/2025 4 - Implement Repository Layer using AWSSDK.DynamoDBv2 - Integrate DI in Aspire AppHost - Design BLL logic 12/11/2025 12/11/2025 5 - Implement CRUD API Controllers - Run unit tests \u0026amp; integration tests locally 13/11/2025 13/11/2025 6 - Work with FE team - Build basic UI (List/Create) - FE integrates Local API and tests E2E 14/11/2025 14/11/2025 Week 10 Achievements Overview This week, I focused on building and validating the entire backend infrastructure in the local environment using Aspire AppHost. I successfully implemented DynamoDB Local, tested CRUD operations, and integrated the backend with the frontend. The stable local setup provides a strong foundation for moving backend services into AWS Lambda and API Gateway in Week 11.\nKnowledge Gained DynamoDB Single-Table Design: PK/SK, access patterns, and efficient item modeling. Setup and usage of DynamoDB Local, Docker, and NoSQL Workbench. Using AWSSDK.DynamoDBv2 in .NET and dependency injection within Aspire AppHost. Backend architecture: Controller → BLL → Repository. API integration with Frontend and handling JSON responses. Practical Work / Deliverables Deployed DynamoDB Local using Docker and modeled schema via NoSQL Workbench. Implemented Repository, BLL, and REST Controllers using .NET 8 / Aspire AppHost. Completed 5–6 CRUD APIs (POST / GET / PUT / DELETE / LIST). Wrote BLL unit tests and API integration tests. Frontend team successfully built List + Create pages and validated end-to-end flow with Local API. "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives Support the team in completing FE–BE integration and ensure API contracts are consistent across environments. Perform end-to-end testing from FE → API Gateway (local mock) → Backend to verify system readiness before cloud deployment. Begin learning and validating the deployment workflow: CI/CD pipeline, Lambda packaging, API Gateway configuration, S3 + CloudFront setup. Prepare DevOps notes for Week 12 documentation (deployment guide, environment setup, troubleshooting). Tasks to Be Carried Out This Week Day Task Start Date End Date 2 - Work with FE team to finalize UI components - Review API contracts and ensure consistent payload \u0026amp; response formats 17/11/2025 17/11/2025 3 - FE integrates full CRUD API flows - Resolve schema mismatches, payload formatting issues, and incorrect status codes 18/11/2025 18/11/2025 4 - Perform full E2E testing: List → Create → Update → Delete - Refine validation rules and response models for eventual Lambda deployment 19/11/2025 19/11/2025 5 - Study deployment workflow: .NET packaging → Lambda, API Gateway routes, CI/CD (GitLab Runner), FE deploy to S3 + CloudFront - Take notes for Week 12 20/11/2025 20/11/2025 6 - Summarize FE–BE integration issues - Review backend codebase to ensure it is ready for cloud migration (env variables, logging, error handling) 21/11/2025 21/11/2025 Week 11 Achievements Overview This week I mainly assisted the FE team in completing the UI and ensuring smooth integration with the backend. As backend logic had been completed in Week 10, my role focused on API contract consistency, bug fixing, and end-to-end validation.\nIn addition, I studied the team’s deployment workflow, including Lambda deployment, API Gateway mapping, CloudFront hosting, and GitLab CI/CD, in preparation for Week 12 documentation.\nKnowledge Acquired How FE interacts with CRUD APIs and how to debug requests/responses. Principles of API contract consistency: input/output schema structure, error model, status code conventions. Deployment workflow overview: Packaging .NET into Lambda (zip bundle, handler configuration). API Gateway routing and authorizer setup. FE hosting via S3 + CloudFront. GitLab CI/CD deployment flow. How to prepare code for deployment: environment variables, structured logging, error handling, configuration separation. Practical Work / Deliverables Worked with FE team to finalize List/Create/Update/Delete screens. Updated backend response schemas, validations, and status codes for FE alignment. Completed end-to-end testing between FE ↔ BE in the local environment. Documented the Lambda + API Gateway + CloudFront deployment workflow for Week 12. Compiled integration issues and added them to the team backlog for tracking. "},{"uri":"https://tuananh3232.github.io/aws_intership/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives After completing local backend development in Week 10 and FE–BE integration in Week 11,\nWeek 12 focuses on building the AWS foundation required for the upcoming cloud deployment (Lambda, API Gateway, DynamoDB, CloudFront).\nThe objectives include:\nUnderstand essential AWS services that will be used in Week 13–14 deployment. Set up and configure an AWS Free Tier account with IAM best practices. Become familiar with AWS Console and AWS CLI for DevOps workflows. Learn basic compute/networking concepts (EC2, EBS, SSH, VPC) to understand cloud infrastructure before moving into serverless deployment. Tasks to Be Carried Out This Week Day Task Start Date End Date 2 - Onboarding with FCJ members - Review rules and DevOps workflow expectations 08/11/2025 08/11/2025 3 - Study AWS core service groups (Compute, Storage, Networking, Database, IAM) - Map these services to the TaskHub architecture 08/12/2025 08/12/2025 4 - Create AWS Free Tier account - Install \u0026amp; configure AWS CLI - Practice listing, querying, and describing resources 08/13/2025 08/13/2025 5 - Learn EC2 fundamentals: AMI, instance types, EBS, Security Groups, Elastic IP - Understand cloud networking basics (VPC/Subnets) 08/14/2025 08/15/2025 6 - Hands-on practice: launch EC2, connect via SSH, attach EBS volume - Compare Console vs CLI operations from a DevOps perspective 08/15/2025 08/15/2025 Week 12 Achievements Overview Following Week 10 (local development) and Week 11 (integration testing),\nthis week builds the cloud knowledge foundation needed before migrating backend services to AWS.\nI learned how AWS organizes its services, how DevOps engineers operate resources using Console and CLI,\nand prepared my environment for real deployment tasks in Week 13.\nKnowledge Acquired AWS Fundamentals for DevOps\nUnderstanding how Compute, Storage, Networking, Database, and IAM support cloud infrastructure. Mapping EC2/VPC knowledge to Lambda and API Gateway architecture. AWS Account \u0026amp; IAM Setup\nCreated Free Tier account. Configured IAM user, Access Key, Secret Key. Set default region and profile for CLI. AWS CLI Essentials\nChecking account configurations. Listing regions, services, and EC2 metadata. Creating/using key pairs. Managing resources through CLI (DevOps workflow). EC2 Hands-on Skills\nLaunched an EC2 instance. Connected via SSH. Attached and managed EBS volumes. Understood VM lifecycle compared to serverless compute. Practical Deliverables Fully configured AWS Free Tier account and IAM setup for DevOps tasks. AWS CLI configuration ready for deployment pipelines. Hands-on practice with EC2 to understand cloud compute before switching to Lambda. Notes summarizing AWS foundational concepts to support Week 13 deployment. "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.17-secretmanager/","title":"Configure AWS Secrets Manager","tags":[],"description":"","content":"Objective Use AWS Secrets Manager to store configuration/secrets for the TaskHub system with the following requirements:\nSecrets are stored in JSON format (Key/value pairs – Plaintext) Data is encrypted using KMS CMK: taskhub_kms (Optional) Enable automatic secret rotation using AWS Lambda Step 1 – Access AWS Secrets Manager In the AWS Console, type Secrets Manager in the search box. Select Secrets Manager from the Services list. Step 2 – Create a New Secret (Key/value pairs – JSON) On the Secrets Manager main page, click Store a new secret. In Secret type, select: Other type of secret.\nIn the Key/value pairs section:\nSwitch from the Key/value tab to the Plaintext tab. Paste the following JSON content (demo for DynamoDB + KMS): { \u0026#34;Service\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;Table\u0026#34;: \u0026#34;TaskHub Tables\u0026#34;, \u0026#34;Encryption\u0026#34;: \u0026#34;SSE-KMS\u0026#34;, \u0026#34;KMSKeyAlias\u0026#34;: \u0026#34;taskhub_kms\u0026#34;, \u0026#34;Purpose\u0026#34;: \u0026#34;Store users, projects, tasks\u0026#34;, \u0026#34;DataProtection\u0026#34;: \u0026#34;Encrypted at rest\u0026#34; } In the Encryption key field, select the KMS key:\ntaskhub_kms Click Next.\nStep 3 – Configure Secret Name and Basic Information At the Configure secret step:\nSecret name:\nExample:\nprod/taskhub/secretmanager\nDescription (optional):\nMetadata for TaskHub DynamoDB encryption demo\nTags (optional): Skip for the workshop.\nResource permissions (optional): Keep default (IAM-based access control).\nReplicate secret (optional): Do not enable for this workshop.\nClick Next.\nStep 4 – Configure Automatic Rotation (Optional) In a production environment, secrets are usually rotated every 30 days.\nIn this workshop, a shorter interval is configured for demonstration purposes.\nAt Configure rotation – optional, enable: Automatic rotation In Rotation schedule: Select Schedule expression builder Time unit: Hours Hours: 23 (Optional) Window duration: 4h Keep Rotate immediately when the secret is stored checked In Rotation function: Select the Lambda function: taskhub-backend Click Next. Step 5 – Review \u0026amp; Store the Secret On the Review step, verify the following information:\nSecret type: Other type of secret Encryption key: taskhub_kms Secret name: prod/taskhub/metadata Automatic rotation: Enabled Lambda rotation function: taskhub-backend Scroll down to the Sample code section:\nAWS provides built-in sample getSecret() functions for: Java JavaScript Python C# Go The TaskHub backend will use the corresponding SDK to retrieve secrets from AWS Secrets Manager instead of hard-coding them in source code. Click Store to complete the process.\nResult A new secret has been successfully created in AWS Secrets Manager. The secret content is stored in JSON format. The secret is: Encrypted at rest using KMS (taskhub_kms) Automatically rotatable using AWS Lambda The TaskHub backend can retrieve secrets via: AWS SDK IAM Role / Policy "},{"uri":"https://tuananh3232.github.io/aws_intership/5-workshop/5.18-waf/","title":"Set up Web ACL","tags":[],"description":"","content":"Step 1 – Access AWS WAF \u0026amp; Shield Sign in to the AWS Console Search for the service: WAF \u0026amp; Shield Click AWS WAF\nFrom the left menu, select:\nProtection packs (web ACLs)\nClick the button:\nCreate protection pack (web ACL)\nStep 2 – Select Application Type (Tell us about your app) 2.1 Select App Category Choose:\nAPI \u0026amp; integration services\nStep 3 – Select the CloudFront Distribution to Protect Expand Select resources to protect Click Add resources Select:\nGlobal → Add CloudFront or Amplify resources Check the CloudFront distribution of TaskHub (S3 frontend) Click Add Step 4 – Select the Rule Pack Type Select: ✅ Build your own pack from all of the protections AWS WAF offers\nIn the right panel, select: ✅ AWS-managed rule group\nClick Next Step 5 – Add Amazon IP Reputation List Select the rule: In Rule overrides, configure as follows: Rule Action AWSManagedIPReputationList ✅ Block AWSManagedReconnaissanceList ✅ Block AWSManagedIPDDoSList ✅ Count Click Add rule ✅ After this step, the rule will appear in the Add rules list.\nStep 6 – Verify the Added Rule Verify that the following information appears:\nRule: AWSManagedRulesAmazonIpReputationList Status: Saved WCU: 25 WCU Step 7 – Set the Web ACL Name In the Name and describe section:\nName: taskhub-waf Description: (leave blank or enter any description) Step 8 – Create the Protection Pack (Web ACL) Click: Create protection pack (web ACL)\nWait for AWS to complete the Web ACL creation "},{"uri":"https://tuananh3232.github.io/aws_intership/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tuananh3232.github.io/aws_intership/tags/","title":"Tags","tags":[],"description":"","content":""}]